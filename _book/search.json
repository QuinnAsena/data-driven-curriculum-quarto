[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Palaeoecological data-science: a data driven curriculum",
    "section": "",
    "text": "1 Introduction\nWelcome to palaeoecological data-science! This book is designed as a reproducible workflow for a semester-long course in palaeoecological data-science with the accompany publication XXX. For students, the book is carefully designed as both an educational resource with working examples and exercises, and as a coding resource that can be transferred to your own analyses. The cautionary note being that the most appropriate analyses (e.g., NMDS or GAMs?), or parameterisation of the analyses will likely be different for your data. To help, the key resources associated with each analytical method are cited throughout. Some R knowledge is assumed but we have tried to make it as easy-to-follow as possible with reproducible, working code chunks.\nFor instructors, the repository can be forked and edited to suit different needs. The pollen data from Devilâ€™s Lake (Wisconsin, USA) used as an example dataset can be swapped out for a different region or proxy (e.g., diatoms) and the workflow edited to accommodate for the change. We would love to know how the resource is adapted and used so drop us an email and let us know!\nIf you do use this resource for teaching or learning please cite it: DOI XXX. This will help us keep track of the impact and use of such open-source curricula and inform future work.\nThis curriculum is an open-source and we welcome suggestions for improvement from the community that will be reviewed by the developers. We will also keep the resource up-to-date with the latest package improvements. The development of this curriculum is tracked with version control using git and the renv package is used to control package versions. section perhaps should be edited and moved to readme with instructions for raising github issues"
  },
  {
    "objectID": "index.html#what-this-book-is",
    "href": "index.html#what-this-book-is",
    "title": "Palaeoecological data-science: a data driven curriculum",
    "section": "1.1 What this book is",
    "text": "1.1 What this book is\nMany palaeo research projects follow the same general workflow, of course there will always be unique questions, challenges, and additional interests. This book is intended to cover the general workflow from obtaining data, through wrangling, analysing and visualising. There are examples throughout"
  },
  {
    "objectID": "index.html#what-this-book-is-not",
    "href": "index.html#what-this-book-is-not",
    "title": "Palaeoecological data-science: a data driven curriculum",
    "section": "1.2 What this book is not",
    "text": "1.2 What this book is not\nPalaeo-science comprise many disciplines from biochemistry and climatology, to mathematics, and ecology. We focus on providing a workflow but cannot, in detail, cover every challenge faced in, for example, age-depth modelling or statistical analyses of proxies. To remedy this shortcoming, each section has the key references that, while not essential for this book, are a starting point to understanding essential challenges."
  },
  {
    "objectID": "index.html#is-this-going-to-hurt",
    "href": "index.html#is-this-going-to-hurt",
    "title": "Palaeoecological data-science: a data driven curriculum",
    "section": "1.3 Is this going to hurt?",
    "text": "1.3 Is this going to hurt?\nYes. But it will be worth it. Palaeoecological data are complex and demand advanced analyses and modelling methods that require anything from basic coding proficiency and a laptop, to super-computing resources and online databases.\n\nSet up group Zotero\nComplete writing\nComplete YAML file\nAuthor order? Authors per chapter?\nrenv\nreplace PSM with LDA?\nhost and track with google analytics?\ncheck literature is open access?"
  },
  {
    "objectID": "01-intro.html#lab-overview",
    "href": "01-intro.html#lab-overview",
    "title": "2Â  Neotoma Explorer",
    "section": "2.1 Lab overview",
    "text": "2.1 Lab overview\nThis lab is designed to introduce you to the Neotoma Paleoecology Database and Neotoma Explorer. Neotoma DB is an open and community-curated resource for paleoecological data. Open means that anyone can find and download data from Neotoma. Community-curated means that Neotoma data are augmented and maintained by a community of experts, with many scientists contributing their data to Neotoma and actively working to curate and improve these data. Explorer is a graphical interface that is very handy for a first-pass search, discovery, and viewing of paleoecological data, typically exploring a few sites at a time. In following weeks, we will move to more advanced visualizations and analyses of paleoecological data, using software designed to download data from Neotoma and analyze it."
  },
  {
    "objectID": "02-intro-to-R.html#a-necessary-evil",
    "href": "02-intro-to-R.html#a-necessary-evil",
    "title": "3Â  Intro to R",
    "section": "3.1 A necessary evil",
    "text": "3.1 A necessary evil\nThis course assumes a degree of knowledge of the R language (e.g., packages and data structures), and principles of tidy data (e.g., long and wide data format). We include detailed comments in the code, but do not attempt to provide a complete introduction to R. We include pointers to exceptional resources for learning R basics should you need them:\nFree books by Roger Peng and Hadley Wickham\n\n(Telford biostats, environmental computing)"
  },
  {
    "objectID": "02-intro-to-R.html#r-gui",
    "href": "02-intro-to-R.html#r-gui",
    "title": "3Â  Intro to R",
    "section": "3.2 R GUI",
    "text": "3.2 R GUI\nRStudio and VS Code"
  },
  {
    "objectID": "02-intro-to-R.html#r-projects",
    "href": "02-intro-to-R.html#r-projects",
    "title": "3Â  Intro to R",
    "section": "3.3 R projects",
    "text": "3.3 R projects\nR projects are not essential to use but can make life a little easier"
  },
  {
    "objectID": "02-intro-to-R.html#directory-structure",
    "href": "02-intro-to-R.html#directory-structure",
    "title": "3Â  Intro to R",
    "section": "3.4 Directory structure",
    "text": "3.4 Directory structure\nHaving a well structured project directory is important for accessing the files you need and for reproducibility."
  },
  {
    "objectID": "02-intro-to-R.html#version-control",
    "href": "02-intro-to-R.html#version-control",
    "title": "3Â  Intro to R",
    "section": "3.5 Version control",
    "text": "3.5 Version control\nIt is good practice to back-up your data and use version control for your code. Onve you integrate version control into your workflow you will wonder how you ever lived without it. Version control is a vast subject but knowing the basics can get you a very far. The Git language (not to be confused with GitHub the remote server) is a common method used to colaborate in software development. Git is not a pre-requisite for this book but as a necessary part of an advanced workflow we include some links to learning basic version control:\n\nlinksâ€¦\n\nAs an example, this book itself is version controlled and collaborated on using Git and GitHub, the resources can be found here-LINK"
  },
  {
    "objectID": "02-intro-to-R.html#packages",
    "href": "02-intro-to-R.html#packages",
    "title": "3Â  Intro to R",
    "section": "3.6 Packages",
    "text": "3.6 Packages\nR is equiped with packages for data wrangling aside from the BASE R language, notably data.table and several packages inside the tidyverse library. We wonâ€™t debate the differences here. We will primarily use packages from the tidyverse (dplyr, ggplot2...) because they integrate with theneotoma2` package used for interacting with the Neotoma(link) API."
  },
  {
    "objectID": "02-intro-to-R.html#packages-used-in-this-book",
    "href": "02-intro-to-R.html#packages-used-in-this-book",
    "title": "3Â  Intro to R",
    "section": "3.7 Packages used in this book",
    "text": "3.7 Packages used in this book\nThe following packages are required for this book. Run the following code to install them all (installation might take a few minutes!).\nmore to come, mgcv, pca packagesâ€¦..\n\nif (!require(\"pacman\")) install.packages(\"pacman\", repos=\"http://cran.r-project.org\")\npacman::p_load(tidyverse, neotoma2, Bchron)    # Install & load packages"
  },
  {
    "objectID": "03-age-depth-modelling.html#part-1-background",
    "href": "03-age-depth-modelling.html#part-1-background",
    "title": "4Â  Age-depth modelling",
    "section": "4.1 Part 1: Background",
    "text": "4.1 Part 1: Background\nA foundational difference between geology and ecology is that, for geologists, time is an unknown variable that must be estimated with uncertainty. In contrast, most ecologists can assume that the temporal coordinates of their observations are known precisely, with zero uncertainty. Fortunately, geochronologists have a wide variety of natural clocks, thanks to the constant decay rates of radioactive isotopes. Each isotope has a unique decay rate, and so each is appropriate for particular timescales.\nFor the last 50,000 years, radiocarbon (14C), with its half-life of 5,730 years, is by far the most common form of radiometric dating. (Beyond 10 half-lives, so much of a radioactive substance has decayed away that it becomes immeasurable.) Radiocarbon is the mainstay of Quaternary dating and archaeology.\nIn Quaternary paleoecology, radiocarbon dating is expensive â€“ a single sample typically costs $300 to $500 â€“ so usually a given lake-sediment record will have only a scattering (ca. 5 to 30) of radiocarbon dates and other age controls. Other kinds of age controls include volcanic ash layers (tephras), 210Pb (half-life: 22.6 yrs), optically stimulated luminescence (OSL) dates, historic events such as the rise in Ambrosia pollen abundances associated with EuroAmerican land clearance, etc. An age-depth model must be constructed to estimate the age of sediments not directly associated with an age control. In multi-site data syntheses, the number of age controls, their precision, and their likely accuracy are all fundamental indicators of data quality (e.g. Blois et al. 2011; Mottl et al. 2021).\nTo estimate ages for depths lacking radiocarbon date, an age-depth model is required. Age-depth models are fitted to the available age controls (age estimates with uncertainty for individual depths) and provide estimates of age as a function of depth, for all depths and ages within the temporal bounds of the model.\nHere we will gain practice in working with age-depth models of various kind, and assessing their age estimates and uncertainty. Weâ€™ll begin with a bit of practice in calibrating radiocarbon years to calendar years and comparing the resulting age estimates from different calibration curves.\n\n\n\n\n\n\nPackages required for this section\n\n\n\nWe will be using Bchron for calibration and Bayesian age-depth modelling. Notably rbacon is another commonly used package, see Trachsel and Telford (2017) for a discussion on age-depth models. We will also be fitting some interpolation and linear models using BASE-R.\n\n# Load up the package\nlibrary(Bchron)\nlibrary(splines) # for fitting splines comes with Base-R\n\n\n\n\n4.1.1 Calibration of Radiocarbon Dates\nA complication in radiocarbon dating is that the initial calculation of a radiocarbon age assumes, by convention, that the amount of radiocarbon in the atmosphere is constant over time. See Bronk Ramsey (2008) for a good overview of 14C dating. This assumption is untrue, so all radiocarbon age estimates must be post-hoc calibrated using a calibration curve that is based on compiling radiocarbon dates of materials that have precise independent age estimates (e.g.Â tree rings, corals). The IntCal series (IntCal04, IntCal09, IntCal13, IntCal20) is the community standard for calibrating radiocarbon dates to age estimates in calendar years (e.g., Reimer et al. 2020). The conversion from radiocarbon to calendar years usually further increases the uncertainty of age estimates.\nYet another complication in radiocarbon dating is that different calibration curves need to be used for the Northern vs.Â Southern Hemisphere and for the atmosphere vs.Â oceans, due to different residence times of 14C in these different reservoirs. For example, atmospheric radiocarbon that diffuses into the surface oceans usually will reside for centuries before phytoplankton biologically fix it through photosynthesis, which will lead marine 14C to be depleted (and â€˜too oldâ€™) relative to atmospheric 14C. Use the wrong calibration curve and your age estimate will be inaccurate!\n\n4.1.1.1 Calibrating radiocarbon dates in R\nHere weâ€™ll experiment with calibrating radiocarbon dates, using various calibration curves. Radiocarbon dated samples come back from the lab with a radiocarbon age and standard deviation, among other information. These two bits of information are used to calibrate the radiocarbon dates to estimated ages. R packages may have useful vignettes (package tutorials) and built-in datasets that provide handy test templates. The following code is modified from the Bchron vignette.\n\nCodeSummaryPlotsCalibration curve plots\n\n\n\nages = BchronCalibrate(ages=c(3445,11553,7456), \n                        ageSds=c(50,230,110), \n                        calCurves=c('intcal20','intcal20','intcal20'))\n\n\n\n\nsummary(ages)\n\n95% Highest density regions for Date1\n$`94.4%`\n[1] 3572 3834\n\n\n95% Highest density regions for Date2\n$`0.4%`\n[1] 13004 13025\n\n$`77.9%`\n[1] 13059 13874\n\n$`16.4%`\n[1] 13923 14012\n\n\n95% Highest density regions for Date3\n$`94.6%`\n[1] 8022 8423\n\n\n\n\n\nplot(ages)\n\n[[1]]\n\n\n\n\n\n\n[[2]]\n\n\n\n\n\n\n[[3]]\n\n\n\n\n\n\n\n\nplot(ages, includeCal = TRUE, fillCol = 'red')\n\n\n\n\n\n\n\nThe output summary indicates the range of the highest density regions (i.e., the most likely â€˜realâ€™ age range of the sample). The plot function in Bchron outputs a ggplot object of the high density regions of the most likely ages.\n\n\n\n4.1.2 Types of Age-Depth Models\nDifferent kinds of age-depth models exist, each with their own underlying assumptions and behavior. In the list below, #1-4 are classical or traditional forms of age-depth models, but Bayesian models are now the norm. The packages rbacon (usually referred to as â€˜baconâ€™) and Bchron are the current standards for Bayesian age-depth modelling. Before going to bayesian models, weâ€™ll begin with the classics.\n\nLinear interpolation, a.k.a. â€˜connect the dots,â€™ in which straight lines are drawn between each depth-adjacent pair of age controls.\nLinear regression (\\(y=b0~ + b1x\\); \\(y=\\)time and \\(x=\\)depth; \\(b0\\) and \\(b1\\) are constants), in which a single straight line is fitted through the entire set of age controls. In ordinary linear regression (OLS), the chosen line will minimize the y-dimension distances of individual points to the line. Standard OLS assumes that all observations are normally distributed, which is a poor assumption for calibrated radiocarbon dates.\nPolynomials, also fitted to the entire set of age controls (\\(y= b0 + b1x + b2x^2 + b3x^3 + â€¦bnx^n\\)), are an extension of linear regression, with additional terms for \\(x^2\\), \\(x^3\\), etc. Some arbitrary maximum n is chosen, usually in the range of 3 to 5. These are called â€˜third-order polynomials,â€™ â€˜fifth-order polynomials,â€™ etc.\nSplines, which are a special kind of polynomial function that are locally fitted to subsets of the age controls, and then smoothly interpolated between points. (Several different formulas can be used to generate splines; common forms include cubic, smooth, monotonic, and LOWESS).\nBayesian age models (e.g.Â bacon, bchron, oxcal, etc.). Bayesian models differ in detail, but all combine a statistical representation of prior knowledge with the new data (i.e.Â the age controls at a site) to build an age-depth model with posterior estimates of age for any given depth. Bayesian models are now widely used because\n\nthey allow the incorporation of prior knowledge (e.g., from coring many lakes, we now have decent estimates of typical sediment accumulation rates, Goring et al. (2012));\nthey can handle highly irregular probability distribution functions such as those for radiocarbon dates after calibration; and as a result\nthey generally do a better job of describing uncertainty than traditional age-depth models.\n\n\n\n4.1.2.1 Classical age-depth models\nClassical models are now out-dated methods (Blaauw et al. 2018), but it is useful to understand how they work as literature before the relatively recent development of Bayesian methods has relied on them. Letâ€™s explore some classical methods of age-depth modelling using one of the datasets included with the Bchron package. The data are from a core in Northern Ireland; Sluggan Bog Smith and Goddard (1991), and can be called via:\n\ndata(Sluggan) # Call the data from Bchron\nprint(Sluggan) # Check out the data\n\n\nLinear interpolationLinear regressionPolynomial regressionCubic splines\n\n\nLinear interpolation predicts missing ages by simply, drawing a line between dated samples. This method assumes that there is a constant age-depth relationship between successive samples. An assumption that is unlikely to be true, especially of cores with fewer dated samples than Sluggan Moss.\n\ninterp_ages <- approx(x = Sluggan$ages, y = Sluggan$position) # use the function approx() to interpolate between ages\nplot(x = interp_ages$x, y = interp_ages$y, type = 'l') # Plot the interpolated data\npoints(x = Sluggan$ages, y = Sluggan$position, cex = 1.5, col = 'red') # overlay the original age points\n\n\n\n\n\n\nLinear regression provides a line of best fit through the dated samples. This method assumes a constant age-depth relationship across all samples, also unlikely to be true depending on processes affecting the core during its formation.\n\nmod_ages <- lm(Sluggan$position ~ Sluggan$ages) # Create a linear regression model\nplot(x = Sluggan$ages, y = Sluggan$position, cex = 1.5, col = 'red') # Plot the original ages\nabline(mod_ages) # add the regression line from the regression model\n\n\n\n\n\n\nPolynomial regression allows a curve to be fit through the data. The amount the curve â€˜wigglesâ€™ depends on the order of the polynomial fit to the data. Polynomial regression has the risk of being over-fit.\n\nx <- Sluggan$ages # Renaming the variables because the predict function below is fussy about the input name\ny <- Sluggan$position\npoly_ages <- lm(y ~ poly(x, 3))\n\nplot(x = Sluggan$ages, y = Sluggan$position, cex = 1.5, col = 'red')\nage_range <- seq(from = range(Sluggan$ages)[1], to = range(Sluggan$ages)[2], length.out = 250)\nlines(age_range, predict(poly_ages, data.frame(x = age_range)))\n\n\n\n\n\n\nSplines are a class of functions including, for example, smoothing splines or cubic splines. Cubic splines are pieve-wise polynomials locally between â€˜knotsâ€™. That is the data are split into bins that are fit independently using. By default, the bs() function uses a third degree polynomial. Without providing knots the fit will look the same as a third degree polynomial regression.\n\ncubic_ages <- lm(y ~ bs(x, knots = c(1000, 6000, 12000)))\nplot(x = Sluggan$ages, y = Sluggan$position, cex = 1.5, col = 'red')\nlines(age_range, predict(cubic_ages, data.frame(x = age_range)))\n\n\n\n\n\n\n\nBecause classical age-depth modelling is rarely used now we are not going to delve further into the statistical details of the best way of fitting each model (e.g., the number of knots to use for fitting a cubic spline).\nOne of the issues with classical age-depth modelling is that uncertainty decreases with fewer datapoints.\n\n\n4.1.2.2 Bayesian age-depth models\nNow letâ€™s see what the latest methods show for the same dataset.\n\nCodeSummaryPlot\n\n\nNote that all the values provided to the arguments are contained in the Sluggan dataframe. When creating chronologies from your own (or accessed data), you may need to rename them to match your data.\n\nSlugganOut = with(Sluggan, \n               Bchronology(ages=ages,\n                           ageSds=ageSds, \n                           calCurves=calCurves,\n                           positions=position, \n                           positionThicknesses=thickness,\n                           ids=id, \n                           predictPositions=seq(0,518, by=10)))\n\n\n\nThe summary shows for each position (depth) the median and quartiles of the predicted ages for that position.\n\nsummary(SlugganOut)\n\nQuantiles of predicted ages by depth: \n Depth      2.5%      25%     50%      75%     97.5%\n     0   -62.000    75.00   213.0   390.00   692.025\n    10    25.925   225.75   372.0   518.25   767.100\n    20   147.700   375.75   513.0   638.00   812.200\n    30   268.975   527.00   648.5   741.00   872.050\n    40   491.950   716.00   784.0   843.00   927.050\n    50   964.975  1067.00  1130.0  1195.00  1306.000\n    60  1147.925  1266.00  1327.0  1394.00  1522.100\n    70  1397.875  1485.00  1544.0  1609.00  1783.025\n    80  1500.000  1641.75  1732.0  1842.00  2035.025\n    90  1618.000  1801.00  1904.5  1989.00  2172.000\n   100  1832.975  2005.00  2076.5  2164.00  2275.025\n   110  2156.975  2347.75  2474.0  2622.00  2906.100\n   120  2377.925  2704.50  2844.5  2949.00  3121.025\n   130  2969.900  3156.00  3268.0  3398.00  3718.075\n   140  3134.975  3387.75  3554.0  3739.00  4046.225\n   150  3282.725  3662.50  3845.0  4002.00  4276.200\n   160  3600.800  3980.75  4111.5  4236.25  4415.075\n   170  4194.950  4344.75  4421.0  4495.25  4636.025\n   180  4332.000  4487.00  4555.5  4627.00  4758.025\n   190  4530.975  4659.00  4737.0  4809.00  4934.000\n   200  4601.000  4749.75  4831.0  4904.25  5041.125\n   210  4671.000  4831.00  4910.5  4987.00  5120.000\n   220  4748.975  4917.00  4989.5  5061.00  5205.000\n   230  4909.950  5036.00  5088.0  5153.25  5287.025\n   240  5546.950  5633.00  5674.0  5720.00  5836.050\n   250  5644.000  5732.00  5787.0  5845.00  5965.025\n   260  5712.975  5819.75  5880.5  5935.00  6028.000\n   270  5820.925  5939.00  5987.5  6031.25  6115.000\n   280  6289.975  6480.75  6601.5  6797.00  7249.150\n   290  6538.950  6951.25  7134.5  7305.25  7525.050\n   300  7503.000  7622.75  7692.0  7773.25  8034.050\n   310  7646.000  7845.00  7971.0  8096.00  8371.050\n   320  7803.925  8075.75  8220.5  8353.25  8571.050\n   330  8102.825  8396.00  8483.0  8576.75  8765.025\n   340  8471.975  8608.75  8688.5  8770.00  8970.325\n   350  8547.000  8705.00  8792.0  8882.00  9073.025\n   360  8629.000  8806.00  8898.0  8984.00  9142.075\n   370  9067.925  9219.00  9294.0  9379.25  9516.050\n   380  9160.000  9312.00  9393.0  9469.00  9641.050\n   390  9217.975  9391.00  9469.0  9554.00  9764.050\n   400  9306.925  9468.75  9545.0  9642.00  9870.050\n   410  9497.950  9644.75  9738.0  9871.25 10117.100\n   420  9646.950  9847.00  9974.0 10107.25 10267.050\n   430  9850.925 10067.75 10259.0 10312.25 10457.050\n   440  9980.900 10172.00 10309.5 10377.25 10534.050\n   450 10414.950 10578.00 10677.0 10796.00 11034.100\n   460 10696.925 10914.75 11027.5 11114.25 11309.100\n   470 11124.850 11518.75 11730.5 11977.50 12415.050\n   480 11568.500 12215.75 12421.0 12560.00 12710.050\n   490 12762.000 12867.00 12924.0 12980.00 13124.000\n   500 12927.000 13052.00 13119.0 13199.00 13376.025\n   510 13324.950 13506.75 13597.0 13711.25 13915.025\n\n\n\n\n\nplot(SlugganOut)\n\n\n\n\n\n\n\nA more complete version of the modelling process using Bchron can be found in the vignette. Age-depth modelling is complicated, there are many pitfalls, assumptions, and uncertainties that are often ignored. Recent developments have begun to focus on quantifying uncertainties to understand the reliability of inferences made from the data. In each section we include the key papers on the topic.\n\n\n\n\n\n\nResources\n\n\n\n\nBlaauw et al. (2018)\nParnell et al. (2008)\nTrachsel and Telford (2017)"
  },
  {
    "objectID": "03-age-depth-modelling.html#part-2-exercises",
    "href": "03-age-depth-modelling.html#part-2-exercises",
    "title": "4Â  Age-depth modelling",
    "section": "4.2 Part 2: Exercises",
    "text": "4.2 Part 2: Exercises\nCalibration\n\nModify the example to calibrate the dates to different calibration curves\n\nClassical age-depth modelling:\n\nLoad up the other dataset provided by Bchron and tryâ€¦\nplot 3 different orders of polynomial in different colours"
  },
  {
    "objectID": "03-age-depth-modelling.html#learning-outcomes-rephraseexclude",
    "href": "03-age-depth-modelling.html#learning-outcomes-rephraseexclude",
    "title": "4Â  Age-depth modelling",
    "section": "4.3 Learning outcomes (Rephrase/exclude)",
    "text": "4.3 Learning outcomes (Rephrase/exclude)\n\nLearn how to access and use-built in datasets\nLearn how to find package vignettes\n\n\n\n\n\nBlaauw, Maarten, J. AndrÃ©s Christen, K. D. Bennett, and Paula J. Reimer. 2018. â€œDouble the Dates and Go for Bayes Impacts of Model Choice, Dating Density and Quality on Chronologies.â€ Quaternary Science Reviews 188 (May): 58â€“66. https://doi.org/10.1016/j.quascirev.2018.03.032.\n\n\nBlois, Jessica L., John W. (Jack) Williams, Eric C. Grimm, Stephen T. Jackson, and Russell W. Graham. 2011. â€œA Methodological Framework for Assessing and Reducing Temporal Uncertainty in Paleovegetation Mapping from Late-Quaternary Pollen Records.â€ Quaternary Science Reviews 30 (15-16): 1926â€“39. https://doi.org/10.1016/j.quascirev.2011.04.017.\n\n\nGoring, S., J. W. Williams, J. L. Blois, S. T. Jackson, C. J. Paciorek, R. K. Booth, J. R. Marlon, M. Blaauw, and J. A. Christen. 2012. â€œDeposition Times in the Northeastern United States During the Holocene: Establishing Valid Priors for Bayesian Age Models.â€ Quaternary Science Reviews 48 (August): 54â€“60. https://doi.org/10.1016/j.quascirev.2012.05.019.\n\n\nMottl, OndÅ™ej, Suzette G. A. Flantua, Kuber P. Bhatta, Vivian A. Felde, Thomas Giesecke, Simon Goring, Eric C. Grimm, et al. 2021. â€œGlobal Acceleration in Rates of Vegetation Change over the Past 18,000 Years.â€ Science 372 (6544): 860â€“64. https://doi.org/10.1126/science.abg1685.\n\n\nParnell, A. C., J. Haslett, J. R. M. Allen, C. E. Buck, and B. Huntley. 2008. â€œA Flexible Approach to Assessing Synchroneity of Past Events Using Bayesian Reconstructions of Sedimentation History.â€ Quaternary Science Reviews 27 (19-20): 1872â€“85. https://doi.org/10.1016/j.quascirev.2008.07.009.\n\n\nReimer, Paula J, William E N Austin, Edouard Bard, Alex Bayliss, Paul G Blackwell, Christopher Bronk Ramsey, Martin Butzin, et al. 2020. â€œThe IntCal20 Northern Hemisphere Radiocarbon Age Calibration Curve (0 Cal kBP).â€ Radiocarbon 62 (4): 725â€“57. https://doi.org/10.1017/RDC.2020.41.\n\n\nSmith, A. G., and I. C. Goddard. 1991. â€œA 12500 Year Record of Vegetational History at Sluggan Bog, Co. Antrim, N. Ireland (Incorporating a Pollen Zone Scheme for the Non-Specialist).â€ The New Phytologist 118 (1): 167â€“87.\n\n\nTrachsel, Mathias, and Richard J Telford. 2017. â€œAll Agedepth Models Are Wrong, but Are Getting Better.â€ The Holocene 27 (6): 860â€“69. https://doi.org/10.1177/0959683616675939."
  },
  {
    "objectID": "04-neotoma2.html#introduction",
    "href": "04-neotoma2.html#introduction",
    "title": "5Â  The neotoma2 package",
    "section": "5.1 Introduction",
    "text": "5.1 Introduction\nThis series of exercises is designed to give you hands-on practice in using APIs and the neotoma2 R package (Goring et al, 2015), both for practical reasons and for insights into how open-data systems work. The primary purpose of neotoma2 is to pass data from the Neotoma Paleoecology Database (Neotoma DB) server into your local R environment. Neotoma relies on Application Programming Interfaces (APIs) to communicate with the Neotoma Paleoecology Database, so weâ€™ll begin with an introduction to APIs.\nMuch of this exercise is based on A Simple Workflow and A Not so Simple Workflow tutorials that were originally developed for a workshop convened by the European Pollen Database in June 2022. Today weâ€™ll keep the initial analyses relatively simple, so that you can focus on learning the data structures and functions used by neotoma2. A hard-won lesson for a practicing data scientist is how much time and attention goes into data handling!\nneotoma2 is available from GitHub and can be installed in R using the devtools package via:\ndevtools::install_github('NeotomaDB/neotoma2')\nlibrary(neotoma2)\nIn this tutorial you will learn how to:\n\nUse the Neotoma API\nSearch for sites using site names and geographic parameters\nFilter results using temporal and spatial parameters\nObtain sample information for the selected datasets\nPerform basic analysis including the use of climate data from rasters\n\nThis series of exercises is designed to give you hands-on practice in using APIs and the neotoma2 R package, both for practical reasons and for insights into how open-data systems work. neotoma2â€™s primary purpose is to pass data from the Neotoma DB server into your local R computing environment. Neotoma relies on Application Programming Interfaces (APIs) to communicate with the Neotoma Paleoecology Database, so weâ€™ll begin with an introduction to APIs."
  },
  {
    "objectID": "04-neotoma2.html#the-wide-wide-world-of-apis",
    "href": "04-neotoma2.html#the-wide-wide-world-of-apis",
    "title": "5Â  The neotoma2 package",
    "section": "5.2 The Wide, Wide World of APIs",
    "text": "5.2 The Wide, Wide World of APIs\nThe Neotoma Paleoecology Database is a relational database, hosted on servers at Penn Stateâ€™s Center for Environmental Informatics. For security reasons, direct access to these servers is quite limited, and available only to a few Neotoma and CEI programmers.\nAPIs offer public access points into Neotoma that anyone can use. Each API is basically a function: you provide the API with a set of operational parameters, and it returns a set of data or metadata. Each API hence is designed to support one particular task or set of tasks; it offers a narrow window into the larger Neotoma DB. REST-ful APIs follow a particular set of standards that allow them to be read by web browsers (i.e.Â within the HTTP protocol) and return data objects, typically in HTML, XML, JSON or other human- & machine-readable formats.\nThe Neotoma APIs provide a series of functions for retrieving different kinds of data from Neotoma DB. Data objects are returned in JSON format. For this exercise, we strongly recommend adding an extension to your browser that formats the JSON output to make it easier to read, such as JSONView for Firefox, and JSON Formatter for Chrome.\nThe APIs for Neotoma can be found here: https://api.neotomadb.org/api-docs/. Look through the lists of different APIs and find the one labeled GET /v2.0/data/sites/{siteid}. Then, click on the Try It Out button at right, enter 666 into the site identifier box, and click Execute. Then scroll down. You should see some example code in curl format (ignore this) and as a URL. If you scroll down further, youâ€™ll see the actual data return, in JSON format. JSON is a structured data format designed to be both human-readable and machine-readable. It looks like a nested series of lists.\nNow, letâ€™s go from the API sandbox to direct use of the API URLs in a browser. Copy and paste the below URL into the URL box in your browser:\nhttps://api.neotomadb.org/v2.0/data/sites?sitename=%devil%\nThis should open a new web page in your browser with a returned JSON object. For this search, the JSON object should include 16 or more sites with the name â€˜devilâ€™ in them (note the use of % as wildcards), including Devilâ€™s Lake, WI. The opening line â€œstatusâ€ = â€œsuccessâ€ means that the API ran successfully. Note that it is possible for an API to run successfully but return no data! For example, try:\nhttps://api.neotomadb.org/v2.0/data/sites?sitename=devil\nHere, â€œstatusâ€ = â€œsuccessâ€ but data=[], i.e.Â the API successfully reported back to you that no sites in Neotoma have the exact name of â€˜devilâ€™.\nOnce you know what an API is, you will quickly see how widely these are used by modern browsers and websites. For example, try:\nhttps://www.google.com/search?q=neotoma&ei=-xE_Y4XHA8GF0PEPi6-p-AU&ved=0ahUKEwjFsPSAksz6AhXBAjQIHYtXCl8Q4dUDCA0&uact=5&oq=neotoma&gs_lcp=Cgdnd3Mtd2l6EAMyDgguEIAEEIsDEKgDEJ4DMggILhCABBCLAzIICC4QgAQQiwMyCAguEIAEEIsDMggILhCABBCLAzIICAAQkQIQiwMyCAguEIAEEIsDMggILhCABBCLAzIICC4QgAQQiwMyCAgAEIAEEIsDOgoIABBHENYEELADOgcIABCwAxBDOg0IABDkAhDWBBCwAxgBOgwILhDIAxCwAxBDGAI6DwguENQCEMgDELADEEMYAjoOCC4QgAQQxwEQ0QMQ1AI6CwgAEIAEELEDEIMBOhEILhCABBCxAxCDARDHARDRAzoOCC4QgAQQsQMQgwEQ1AI6FAguEIAEELEDEIMBENQCEJsDEKgDOg4ILhCABBDHARDRAxCLAzoOCAAQgAQQsQMQgwEQiwM6EQguEIAEENQCEIsDEKgDEKQDOg4IABCRAhCLAxCmAxCoAzoNCAAQQxCLAxCoAxCmAzoFCAAQgAQ6CAguELEDEIMBOgsILhCABBDHARCvAToHCAAQQxCLAzoICC4QgAQQsQM6CAgAEIAEELEDOhEILhCABBCxAxDUAhCjAxCoAzoLCC4QsQMQgwEQiwM6BwgAEIAEEAo6DggAELEDEIMBEJECEIsDOhAIABCABBCxAxCDARAKEIsDOgoIABCABBAKEIsDOgUIABCRAkoECEEYAEoECEYYAVC5BVirCmDFDGgBcAF4AIABjAGIAdcFkgEDMy40mAEAoAEByAERuAEDwAEB2gEGCAEQARgJ2gEGCAIQARgI&sclient=gws-wiz\nNote that in this demo use of the Google search API, all we did is enter â€˜neotomaâ€™ in the standard Google search window and then Google produced and formatted the above API query with a ?search parameter (and who knows what else is lurking in that massive text parameterâ€¦)\nOK, now your turn:\nExercise Question 1 Use the sites API to retrieve site data for sites of interest. The sites API has a few different parameters, so try out options. Try copying and pasting the URLs into the address line of your browser, and then hitting return. In your homework exercise, provide at least two sites API calls (as URLs) with a comment line for each that explains what the API command is doing."
  },
  {
    "objectID": "04-neotoma2.html#getting-started-with-neotoma2",
    "href": "04-neotoma2.html#getting-started-with-neotoma2",
    "title": "5Â  The neotoma2 package",
    "section": "5.3 Getting Started With neotoma2",
    "text": "5.3 Getting Started With neotoma2"
  },
  {
    "objectID": "05-wrangling-visualisation.html",
    "href": "05-wrangling-visualisation.html",
    "title": "6Â  Data wrangling and visualisations",
    "section": "",
    "text": "Data checks\nTidying data\nPlotting data"
  },
  {
    "objectID": "06-exploratory-analyses.html",
    "href": "06-exploratory-analyses.html",
    "title": "7Â  Exploratory Analyses",
    "section": "",
    "text": "Basic descriptors of palaeoecological data"
  },
  {
    "objectID": "07-dissimilarity-roc.html",
    "href": "07-dissimilarity-roc.html",
    "title": "8Â  Dissimilarity and Rates of Change",
    "section": "",
    "text": "Here is an equation.\n\\[\\begin{equation}\n  f\\left(k\\right) = \\binom{n}{k} p^k\\left(1-p\\right)^{n-k}\n  (\\#eq:binom)\n\\end{equation}\\]"
  },
  {
    "objectID": "08-no-analogue.html",
    "href": "08-no-analogue.html",
    "title": "9Â  Novelty and No Analogue Futures",
    "section": "",
    "text": "Jack"
  },
  {
    "objectID": "09-ordination-pca.html",
    "href": "09-ordination-pca.html",
    "title": "10Â  Ordination: Principal Component Analysis",
    "section": "",
    "text": "Principal Component Analysis (PCA) is a statistical method for reducing \\(n\\)-dimensional data (e.g., the \\(n\\) species in your data) to fewer axes that minimise the variance in the data."
  },
  {
    "objectID": "10-ordination-nmds.html",
    "href": "10-ordination-nmds.html",
    "title": "11Â  Ordination: Non-metric Dimension Scaling",
    "section": "",
    "text": "I need wifi."
  },
  {
    "objectID": "11-gams.html#hierarchical-generalised-additive-models.",
    "href": "11-gams.html#hierarchical-generalised-additive-models.",
    "title": "12Â  Generalised Additive Models",
    "section": "12.1 Hierarchical Generalised Additive Models.",
    "text": "12.1 Hierarchical Generalised Additive Models."
  },
  {
    "objectID": "12-tapas.html",
    "href": "12-tapas.html",
    "title": "13Â  Charcoal Analysis using TAPAS",
    "section": "",
    "text": "Tapas is a R package for analysing charcoal"
  },
  {
    "objectID": "13-psm.html",
    "href": "13-psm.html",
    "title": "14Â  Proxy System Modelling",
    "section": "",
    "text": "Should this be replaced with LDA/CTM?"
  },
  {
    "objectID": "15-test-chapter.html",
    "href": "15-test-chapter.html",
    "title": "15Â  Test pad to delete",
    "section": "",
    "text": "Sandbox chapter for testing scripts outside main writing if necessary\n\n\n\n\n\n\nPackages required for this section\n\n\n\nList of packages required for section and what they are used for\n\n\n\n\n\n\n\n\nRespawn code\n\n\n\nBegin each section with a code-block that can be run to reproduce the necessary data format for the upcoming section\n\n\n\n\n\n\n\n\nOh no! ðŸ˜±\n\n\n\nExample of common mistakes ðŸ˜±\n\n\n\n\n\n\n\n\nGood practice tip\n\n\n\nTips on version control, directory structure (and anything else) that make life easier\n\n\n\n\n\n\n\n\nResources\n\n\n\nList of packages required for section and what they are used for"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Blaauw, Maarten, J. AndrÃ©s Christen, K. D. Bennett, and Paula J. Reimer.\n2018. â€œDouble the Dates and Go for Bayes\nImpacts of Model Choice, Dating Density and Quality on\nChronologies.â€ Quaternary Science Reviews 188 (May):\n58â€“66. https://doi.org/10.1016/j.quascirev.2018.03.032.\n\n\nBlois, Jessica L., John W. (Jack) Williams, Eric C. Grimm, Stephen T.\nJackson, and Russell W. Graham. 2011. â€œA Methodological Framework\nfor Assessing and Reducing Temporal Uncertainty in Paleovegetation\nMapping from Late-Quaternary Pollen Records.â€\nQuaternary Science Reviews 30 (15-16): 1926â€“39. https://doi.org/10.1016/j.quascirev.2011.04.017.\n\n\nGoring, S., J. W. Williams, J. L. Blois, S. T. Jackson, C. J. Paciorek,\nR. K. Booth, J. R. Marlon, M. Blaauw, and J. A. Christen. 2012.\nâ€œDeposition Times in the Northeastern United States\nDuring the Holocene: Establishing Valid Priors for\nBayesian Age Models.â€ Quaternary Science\nReviews 48 (August): 54â€“60. https://doi.org/10.1016/j.quascirev.2012.05.019.\n\n\nMottl, OndÅ™ej, Suzette G. A. Flantua, Kuber P. Bhatta, Vivian A. Felde,\nThomas Giesecke, Simon Goring, Eric C. Grimm, et al. 2021. â€œGlobal\nAcceleration in Rates of Vegetation Change over the Past 18,000\nYears.â€ Science 372 (6544): 860â€“64. https://doi.org/10.1126/science.abg1685.\n\n\nParnell, A. C., J. Haslett, J. R. M. Allen, C. E. Buck, and B. Huntley.\n2008. â€œA Flexible Approach to Assessing Synchroneity of Past\nEvents Using Bayesian Reconstructions of Sedimentation\nHistory.â€ Quaternary Science Reviews 27 (19-20):\n1872â€“85. https://doi.org/10.1016/j.quascirev.2008.07.009.\n\n\nReimer, Paula J, William E N Austin, Edouard Bard, Alex Bayliss, Paul G\nBlackwell, Christopher Bronk Ramsey, Martin Butzin, et al. 2020.\nâ€œThe IntCal20 Northern Hemisphere Radiocarbon Age\nCalibration Curve (0 Cal kBP).â€ Radiocarbon 62 (4): 725â€“57.\nhttps://doi.org/10.1017/RDC.2020.41.\n\n\nSmith, A. G., and I. C. Goddard. 1991. â€œA 12500 Year\nRecord of Vegetational History at Sluggan\nBog, Co. Antrim, N.\nIreland (Incorporating a Pollen Zone\nScheme for the Non-Specialist).â€ The New\nPhytologist 118 (1): 167â€“87.\n\n\nTrachsel, Mathias, and Richard J Telford. 2017. â€œAll Agedepth\nModels Are Wrong, but Are Getting Better.â€ The Holocene\n27 (6): 860â€“69. https://doi.org/10.1177/0959683616675939."
  }
]