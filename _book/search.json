[
  {
    "objectID": "06-dissimilarity.html",
    "href": "06-dissimilarity.html",
    "title": "6  Dissimilarity",
    "section": "",
    "text": "6.1 Part 1: Background\nThere are multiple methods of describing palaeoecological data in terms of species communities. Older, but still valid, methods include CONISS (Grimm 1987), more modern methods include machine learning methods such as boosted regression trees (Elith, Leathwick, and Hastie 2008; Simpson and Birks 2012), and latent dirichlet allocation (Blei and Lafferty 2007; Blei 2012). Here, we are going to look at a measure of dissimilarity, the squared chord distance, of samples through time. That is, a measure of how similar/dissimilar a community at time \\(t\\) is to the community at \\(t_{+1}\\).\nWe will be using the Devil’s Lake dataset downloaded in Chapter 4. We will read in the pre-formatted data from the data directory directly so that this section can be run independently of Chapter 4. We’ve also included a catch-up block of code (below) that shows the process of downloading and formatting the data to maintain a reproducible record of the steps required to start this section.\nWe can skip the respawn code and reads-in the Devil’s Lake data provided in the data directory. Sometimes minor mistakes along the way lead to errors, so you can always come back here and read in the formatted data again.\n# Load up the data\ndevils_samples &lt;- readRDS(\"./data/devils_samples.rds\")\nThe data are in long format as a tibble, i.e., similar to a dataframe. This format can be great for data wrangling and plotting; however, many analyses require a specific input such as a wide (variables in columns) site-by-species matrix. It is important to be comfotable with pivoting data between long and wide formats and converting between dataframes/tibbles and matrices. Remember, that unlike dataframes and tibbles, matrices cannot be of mixed type (e.g., strings and numbers).\nThe analogue package provides several distance measures that can be calculated for site-by-species matrices, we will use squared chord distance. So let’s pivot our data to wide format and convert it to a numeric matrix. This can be done in several ways in R, for consistency’s sake we’ll use tidyr:\ndevils_samples_wide &lt;- devils_samples %&gt;%\n    tidyr::pivot_wider(id_cols = c(age, depth),\n                       names_from = variablename,\n                       values_from = prop,\n                       values_fill = 0) %&gt;% # Replace NA with 0 (see note below) \n    as.matrix()\n\n# Some species were not recorded at certain depths (e.g., Acer at depth 603)\n# one must be careful when replacing NA with 0 as 'not observed' is different to a measurement of 0\n# in this case we can safely say that the proportion of Acer was 0 at a depth of 603\n\ndevils_ad &lt;- devils_samples_wide[ , c(1:2)] # store the ages and depths separately\n\ndevils_samples_wide &lt;- devils_samples_wide[ , -c(1:2)] # drop ages and depth as they are not included in the distance calculations\ndevils_names &lt;- colnames(devils_samples_wide)\nNow we have our data in the correct format to do the distance calculation. Note that time is not included as part of the calculation. The age and depth columns have been stored in a separate matrix devils_ad, and the suqared chord distance will be calculated on the species community matix devils_samples_wide.\nThe distance function has returned a maxtrix of 123 x 123 dimensions. Note that the input dimensions were 123 x 51. The function has calculated the distance between all pairwise samples, that is, every combination of 1 to \\(n\\) samples. The diagonal of the matrix is each sample compared with itself and has values of zero. The upper and lower halves of the matrix mirror each other.\nOne way we can visualise these data is by picking an origin from which to compare change, and re-appending the time column (although, see the cautions below of this crude method). If we use the top of the core, the most recent section, as our origin and we can look at the distance of each following sample from the present. This will be the first column (or row) of the matrix devils_sqdist, so let’s bind the first column with our stored ages and depths, and plot distance over time.\nWe can see from this plot that the community changed substantially in Devil’s Lake from the origin point of the community at the most recent core section. Check out the pattern shown by applying the Modern Analogue Technique in Chapter 7 and how it compares to this plot.\nHowever, plotting the distances with the original time column is not a time series analysis and does not take into account structural processes or autocorrelation. As an exploratory method, it shows us patterns that may be of interest to investigate further.\nSome consiterations of this method are:\nSquared chord distance is one of many measures of dissimilarity, and is not necessarily the best for a given dataset (see Simpson 2007). The documentation of the distance function (accessed by ?distance). The vegan package also includes functions for calculating distance, and provides many useful references in the documentation of the function vegdist.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Dissimilarity</span>"
    ]
  },
  {
    "objectID": "06-dissimilarity.html#part-1-background",
    "href": "06-dissimilarity.html#part-1-background",
    "title": "6  Dissimilarity",
    "section": "",
    "text": "Packages required for this section\n\n\n\n\n# Load up the package\nif (!require(\"pacman\")) install.packages(\"pacman\", repos=\"http://cran.r-project.org\")\npacman::p_load(tidyr, dplyr, ggplot2, neotoma2, analogue, vegan)\n\n\n\n\n\n\n\n\n\n\nRespawn code\n\n\n\nThe following code recreates the necessary formatted data from Devil’s Lake using the neotoma2 package. We have also provided the formatted dataset as an .rds file in the data directory, it is preferable to read the dataset rather than repeatedly download it to avoid unnecessary API calls. This code block is included so that you can manipulate the inputs (e.g., sideid =, and harmonising taxa) and object name to download and experiment with different datasets.\n\n\nCode\n# The following code can be done in one pipeline but we have split it into chunks to help with readablity\ndevils_samples &lt;- get_sites(siteid = 666) %&gt;% # Download the site samples data\n  get_downloads() %&gt;%\n  samples()\n\ndevils_samples &lt;- devils_samples %&gt;%+\n  dplyr::filter(ecologicalgroup %in% c(\"UPHE\", \"TRSH\"), # Filter ecological groups by upland heath and trees and shrubs\n         elementtype == \"pollen\", # filter bu pollen samples\n         units == \"NISP\") %&gt;% # Filter by sampling unit\n  mutate(variablename = replace(variablename,\n                                stringr::str_detect(variablename, \"Pinus.*\"), # Harmonize Pinus into one group\n                                \"Pinus\")) %&gt;%\n  group_by(siteid, sitename,\n           sampleid, variablename, units, age,\n           agetype, depth, datasetid,\n           long, lat) %&gt;%\n  summarise(value = sum(value), .groups='keep') # The group_by function will drop columns not used as grouping variables\n\ndevils_samples &lt;- devils_samples %&gt;% # Calculate proportions of each species by year group\n  group_by(age) %&gt;%\n  mutate(pollencount = sum(value, na.rm = TRUE)) %&gt;%\n  group_by(variablename) %&gt;%\n  mutate(prop = value / pollencount) %&gt;%\n  arrange(desc(age)) %&gt;%\n  ungroup()\n\nsaveRDS(devils_samples, \"./data/devils_samples.rds\") # Save the data for later\n\n\n\n\n\n\n\n\n\n\n\n\n\nOh no! Common sources of error (and anxiety…) 😱\n\n\n\nIf a numeric matrix is required as the input make sure the data have not been accidentally converted to strings. R will coerce numbers to characters if there is a character (like a nameing column) in the matrix. This coersion can go unnoticed when converting from a dataframe to a matrix.\n\nstring_matrix &lt;- matrix(c(\"1\", 2:10), nrow = 2) # see the numbers are in quotes indicating a string? \nstr(string_matrix) # Check the structure\n\n chr [1:2, 1:5] \"1\" \"2\" \"3\" \"4\" \"5\" \"6\" \"7\" \"8\" \"9\" \"10\"\n\n\nNumeric matrix:\n\nnumeric_matrix &lt;- matrix(1:10, nrow = 2) # now the structure is int, for integer.\nstr(numeric_matrix) # Check the structure\n\n int [1:2, 1:5] 1 2 3 4 5 6 7 8 9 10\n\n\nMatrices can have row and column names (e.g., species names) names and maintain a numeric structure, for example: colnames(numeric_matrix) &lt;- LETTERS[1:5].\n\n\n\n\n\n\nCodeResult\n\n\n\n# Here we use method = \"SQchord\", see other available methods with ?distance\ndevils_sqdist &lt;- analogue::distance(devils_samples_wide, method = \"SQchord\")\n\n\n\nPrint the first 5 rows and columns to see the structure:\n\n\n          1          2          3          4          5\n1 0.0000000 0.12303375 0.16807049 0.16763911 0.16863865\n2 0.1230338 0.00000000 0.09150351 0.06611174 0.10351145\n3 0.1680705 0.09150351 0.00000000 0.12078176 0.07946106\n4 0.1676391 0.06611174 0.12078176 0.00000000 0.10291118\n5 0.1686386 0.10351145 0.07946106 0.10291118 0.00000000\n\n\n\n\n\n\n\n\nCodeplot\n\n\n\n# Bind the age and depths variables with the first column of the SQdistances\norigin_top &lt;- cbind(devils_ad, sq_dist = devils_sqdist[ ,1])\n\n\n\n\norigin_top %&gt;%\n  as_tibble() %&gt;% \n  ggplot(aes(x = age, y = sq_dist)) + \n  geom_point() +\n  geom_line() +\n  scale_x_reverse(breaks = scales::pretty_breaks(6)) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nit does not take into account time intervals or chronological uncertainty. Palaeoecological data are typically unevenly spaced through time and we may not be calculating the squared chord distance across communities evenly through time. See the age column in the data, you can use diff() to see the first differences in the data (the timespan between sampels).\ntime averaging is not taken into consideration. Time averaging is the amount of time a sample (i.e., a 1-centemetre thick slice of sediment core) encompasses. Due to processes like variable sedimentation rates and core compression, the number of years inntegrated within each sample may vary. Thus, we may be calculating the dissimilarity between say 50-years worth of ecological productivity and turnover in one sample, with 10-years in the previous one.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Dissimilarity</span>"
    ]
  },
  {
    "objectID": "06-dissimilarity.html#part-2-exercises",
    "href": "06-dissimilarity.html#part-2-exercises",
    "title": "6  Dissimilarity",
    "section": "6.2 Part 2: Exercises",
    "text": "6.2 Part 2: Exercises\nPlot diagonal elements offset by 1?\n\n\n\n\nBlei, David M. 2012. “Probabilistic Topic Models.” Communications of the ACM 55 (4): 77–84. https://doi.org/10.1145/2133806.2133826.\n\n\nBlei, David M., and John D. Lafferty. 2007. “A Correlated Topic Model of Science.” The Annals of Applied Statistics 1 (1). https://doi.org/10.1214/07-AOAS114.\n\n\nElith, J., J. R. Leathwick, and T. Hastie. 2008. “A Working Guide to Boosted Regression Trees.” Journal of Animal Ecology 77 (4): 802–13. https://doi.org/10.1111/j.1365-2656.2008.01390.x.\n\n\nGrimm, Eric C. 1987. “CONISS: A FORTRAN 77 Program for Stratigraphically Constrained Cluster Analysis by the Method of Incremental Sum of Squares.” Computers & Geosciences 13 (1): 13–35. https://doi.org/10.1016/0098-3004(87)90022-7.\n\n\nSimpson, Gavin L. 2007. “Analogue Methods in Palaeoecology: Using the Analogue Package.” Journal of Statistical Software 22 (September): 1–29. https://doi.org/10.18637/jss.v022.i02.\n\n\nSimpson, Gavin L., and H. John B. Birks. 2012. “Statistical Learning in Palaeolimnology.” In Tracking Environmental Change Using Lake Sediments: Data Handling and Numerical Techniques, edited by H. John B. Birks, André F. Lotter, Steve Juggins, and John P. Smol, 249–327. Developments in Paleoenvironmental Research. Dordrecht: Springer Netherlands. https://doi.org/10.1007/978-94-007-2745-8_9.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Dissimilarity</span>"
    ]
  },
  {
    "objectID": "04-neotoma2.html",
    "href": "04-neotoma2.html",
    "title": "4  The neotoma2 package",
    "section": "",
    "text": "4.1 Part 1: Background\nThis series of exercises is designed to give you hands-on practice in using the neotoma2 R package (Goring et al. 2015), both for practical reasons and for insights into how open-data systems work. This section is a condensed version of the current workshop link and focuses on data structures and the core functions in the neotoma2 R package for downloading and manipulating data. A hard-won lesson for a practicing data scientist is how much time and attention goes into data handling! For a more extended introduction to Neotoma, see the current workshop link. If you’re planning on working with Neotoma more in the future, please join us on Slack, where we manage a channel specifically for questions about the R package. You may also wish to join Neotoma’s Google Groups mailing list, and if so contact us to be added.\nThis series of exercises is designed to give you hands-on practice in using APIs and the neotoma2 R package, both for practical reasons and for insights into how open-data systems work. neotoma2’s primary purpose is to pass data from the Neotoma DB server into your local R computing environment.\nIn this tutorial you will learn how to:",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>The `neotoma2` package</span>"
    ]
  },
  {
    "objectID": "04-neotoma2.html#part-1-background",
    "href": "04-neotoma2.html#part-1-background",
    "title": "4  The neotoma2 package",
    "section": "",
    "text": "Search for sites using site names and geographic parameters\nFilter results using temporal and spatial parameters\nObtain sample information for the selected datasets",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>The `neotoma2` package</span>"
    ]
  },
  {
    "objectID": "04-neotoma2.html#the-wide-wide-world-of-apis",
    "href": "04-neotoma2.html#the-wide-wide-world-of-apis",
    "title": "4  The neotoma2 package",
    "section": "4.2 The Wide, Wide World of APIs",
    "text": "4.2 The Wide, Wide World of APIs\nThe Neotoma Paleoecology Database is a relational database, hosted on servers at Penn State’s Center for Environmental Informatics. For security reasons, direct access to these servers is quite limited, and available only to a few Neotoma and CEI programmers.\nAPIs offer public access points into Neotoma that anyone can use. Each API is basically a function: you provide the API with a set of operational parameters, and it returns a set of data or metadata. Each API hence is designed to support one particular task or set of tasks; it offers a narrow window into the larger Neotoma DB. REST-ful APIs follow a particular set of standards that allow them to be read by web browsers (i.e. within the HTTP protocol) and return data objects, typically in HTML, XML, JSON or other human- & machine-readable formats.\nThe Neotoma APIs provide a series of functions for retrieving different kinds of data from Neotoma DB. Data objects are returned in JSON format. For this exercise, we strongly recommend adding an extension to your browser that formats the JSON output to make it easier to read, such as JSONView for Firefox, and JSON Formatter for Chrome.\nThe APIs for Neotoma can be found here: https://api.neotomadb.org/api-docs/. Look through the lists of different APIs and find the one labeled GET /v2.0/data/sites/{siteid}. Then, click on the Try It Out button at right, enter 666 into the site identifier box, and click Execute. Then scroll down. You should see some example code in curl format (ignore this) and as a URL. If you scroll down further, you’ll see the actual data return, in JSON format. JSON is a structured data format designed to be both human-readable and machine-readable. It looks like a nested series of lists.\nNow, let’s go from the API sandbox to direct use of the API URLs in a browser. Copy and paste the below URL into the URL box in your browser:\nhttps://api.neotomadb.org/v2.0/data/sites?sitename=%devil%\nThis should open a new web page in your browser with a returned JSON object. For this search, the JSON object should include 16 or more sites with the name ‘devil’ in them (note the use of % as wildcards), including Devil’s Lake, WI. The opening line “status” = “success” means that the API ran successfully. Note that it is possible for an API to run successfully but return no data! For example, try:\nhttps://api.neotomadb.org/v2.0/data/sites?sitename=devil\nHere, “status” = “success” but data=[], i.e. the API successfully reported back to you that no sites in Neotoma have the exact name of ‘devil’.\nOnce you know what an API is, you will quickly see how widely these are used by modern browsers and websites. For example, try:\nhttps://www.google.com/search?q=neotoma&ei=-xE_Y4XHA8GF0PEPi6-p-AU&ved=0ahUKEwjFsPSAksz6AhXBAjQIHYtXCl8Q4dUDCA0&uact=5&oq=neotoma&gs_lcp=Cgdnd3Mtd2l6EAMyDgguEIAEEIsDEKgDEJ4DMggILhCABBCLAzIICC4QgAQQiwMyCAguEIAEEIsDMggILhCABBCLAzIICAAQkQIQiwMyCAguEIAEEIsDMggILhCABBCLAzIICC4QgAQQiwMyCAgAEIAEEIsDOgoIABBHENYEELADOgcIABCwAxBDOg0IABDkAhDWBBCwAxgBOgwILhDIAxCwAxBDGAI6DwguENQCEMgDELADEEMYAjoOCC4QgAQQxwEQ0QMQ1AI6CwgAEIAEELEDEIMBOhEILhCABBCxAxCDARDHARDRAzoOCC4QgAQQsQMQgwEQ1AI6FAguEIAEELEDEIMBENQCEJsDEKgDOg4ILhCABBDHARDRAxCLAzoOCAAQgAQQsQMQgwEQiwM6EQguEIAEENQCEIsDEKgDEKQDOg4IABCRAhCLAxCmAxCoAzoNCAAQQxCLAxCoAxCmAzoFCAAQgAQ6CAguELEDEIMBOgsILhCABBDHARCvAToHCAAQQxCLAzoICC4QgAQQsQM6CAgAEIAEELEDOhEILhCABBCxAxDUAhCjAxCoAzoLCC4QsQMQgwEQiwM6BwgAEIAEEAo6DggAELEDEIMBEJECEIsDOhAIABCABBCxAxCDARAKEIsDOgoIABCABBAKEIsDOgUIABCRAkoECEEYAEoECEYYAVC5BVirCmDFDGgBcAF4AIABjAGIAdcFkgEDMy40mAEAoAEByAERuAEDwAEB2gEGCAEQARgJ2gEGCAIQARgI&sclient=gws-wiz\nNote that in this demo use of the Google search API, all we did is enter ‘neotoma’ in the standard Google search window and then Google produced and formatted the above API query with a ?search parameter (and who knows what else is lurking in that massive text parameter…)\nOK, now your turn:\nExercise question 1 Use the sites API to retrieve site data for sites of interest. The sites API has a few different parameters, so try out options. Try copying and pasting the URLs into the address line of your browser, and then hitting return. In your homework exercise, provide at least two sites API calls (as URLs) with a comment line for each that explains what the API command is doing.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>The `neotoma2` package</span>"
    ]
  },
  {
    "objectID": "04-neotoma2.html#part-2-getting-started-with-neotoma2",
    "href": "04-neotoma2.html#part-2-getting-started-with-neotoma2",
    "title": "4  The neotoma2 package",
    "section": "4.2 Part 2: Getting Started With neotoma2",
    "text": "4.2 Part 2: Getting Started With neotoma2\nThe primary purpose of neotoma2 is to pass data from the Neotoma Paleoecology Database (Neotoma DB) server into your local R environment. neotoma2 has a layered structure of core functions for obtaining data:\n\nsites\ndatasets\ndownloads\nsamples\n\nThere are many more functions for maipulating data and conducting other operations that we will encounter along the way while we take you through these four data layers. For this workbook we use several packages, including leaflet, sf and others. We load the packages using the pacman package, which will automatically install the packages if they do not currently exist in your set of packages.\n\n\n\n\n\n\nPackages required for this section\n\n\n\n\n# Load up the package\nif (!require(\"pacman\")) install.packages(\"pacman\", repos=\"http://cran.r-project.org\")\npacman::p_load(dplyr, ggplot2, neotoma2, sf, geojsonsf, leaflet, raster, DT, rioja, htmlwidgets) # Pacman will load neotoma2\n\n\n\n\n4.2.1 Good coding practice: explicitly naming packages and functions\nDifferent packages in R are created independently by different teams of developers, and it’s very common for two different packages to use the same function names. This can lead to coding errors if you call a function that you know is in one package, but R guesses wrongly that you wanted a function of the same name from an entirely different package. For example, for a function like filter(), which exists in both neotoma2 and other packages such as dplyr, you may see an error that looks like:\n\n\n\n\n\n\nOh no! 😱\n\n\n\nError in UseMethod(\"filter\") :\n no applicable method for 'filter' applied to an object of class \"sites\"\n\n\nYou can avoid this error by explicitly naming which package has the function that you want to use, through the standard convention of double colons (package.name::function.name). For example, using neotoma2::filter() tells R explicitly that you want to use the filter() function in the neotoma2 package, not some other package version.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>The `neotoma2` package</span>"
    ]
  },
  {
    "objectID": "04-neotoma2.html#neotoma2-site-searches-get_sites",
    "href": "04-neotoma2.html#neotoma2-site-searches-get_sites",
    "title": "4  The neotoma2 package",
    "section": "4.3 Neotoma2: Site Searches: get_sites()",
    "text": "4.3 Neotoma2: Site Searches: get_sites()\nMany users of Neotoma first want to search and explore data at the site level. There are several ways to find sites in neotoma2, but we think of sites primarily as spatial objects. They have names, locations, and are found within geopolitical units. However, sites themselves do not have associated information about taxa, dataset types, or ages. sites instead are simply the container into which we add that information. So, when we search for sites we can search by:\n\nsiteid\nsitename\nlocation\naltitude (maximum and minimum)\ngeopolitical unit\n\n\n4.3.0.1 Searching by Site Name: sitename=\"%Devil%\"\nWe may know exactly what site we’re looking for (“Devil’s Lake”), or have an approximate guess for the site name (for example, we know it’s something like “Devil Pond”, or “Devil’s Hole”).\nWe use the general format: get_sites(sitename=\"XXXXX\") for searching by name.\nPostgreSQL (and the API) uses the percent sign as a wildcard. So \"%Devil%\" would pick up “Devils Lake” for us (and would pick up “Devil’s Canopy Cave”). Note that the search query is case insensitive, so \"%devil%\" will work.\nIf we want an individual record we can use the siteid, which is a unique identifier for each site: .\n\nCodeResult\n\n\n\ndevil_sites &lt;- neotoma2::get_sites(sitename = \"%Devil%\")\nplotLeaflet(devil_sites)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4.3.0.2 Searching by Location: loc=c()\nThe neotoma package used a bounding box for locations, structured as a vector of latitude and longitude values: c(xmin, ymin, xmax, ymax). The neotoma2 R package supports both this simple bounding box and also more complex spatial objects, using the sf package. Using the sf package allows us to more easily work with raster and polygon data in R, and to select sites using more complex spatial objects. The loc parameter works with the simple vector, WKT, geoJSON objects and native sf objects in R. Note however that the neotoma2 package is a wrapper for a simple API call using a URL (see APIs above), and URL strings have a maximum limit of 1028 characters, so the API currently cannot accept very long/complex spatial objects.\nAs an example of different ways that you can search by location, let’s say you wanted to search for all sites in the state of Michigan. Here are three spatial representations of Michigan: 1) a geoJSON list with five elements, 2) WKT, and 3) bounding box representation. And, as a fourth variant, we’ve transformed the mich$geoJSON element to an object for the sf package. Any of these four spatial representations work with the neotoma2 package.\n\nmich &lt;- list(geoJSON = '{\"type\": \"Polygon\",\n        \"coordinates\": [[\n            [-86.95, 41.55],\n            [-82.43, 41.55],\n            [-82.43, 45.88],\n            [-86.95, 45.88],\n            [-86.95, 41.55]\n            ]]}',\n        WKT = 'POLYGON ((-86.95 41.55,\n                         -82.43 41.55,\n                         -82.43 45.88,\n                         -86.95 45.88,\n                         -86.95 41.55))',\n        bbox = c(-86.95, 41.55, -82.43, 45.88))\n\nmich$sf &lt;- geojsonsf::geojson_sf(mich$geoJSON)[[1]]\n\nmich_sites &lt;- neotoma2::get_sites(loc = mich$geoJSON, all_data = TRUE)\n\nYou can always simply plot() the sites objects, but this won’t show any geographic context. The leaflet::plotLeaflet() function returns a leaflet() map, and allows you to further customize it, or add additional spatial data (like our original bounding polygon, mich$sf, which works directly with the R leaflet package):\n\nCodeResult\n\n\n\nneotoma2::plotLeaflet(mich_sites) %&gt;%\n  leaflet::addPolygons(map = .,\n                       data = mich$sf,\n                       color = \"green\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4.3.0.3 Helper Functions for Site Searches\n\n\n\nNeotoma R Package Universal Modeling Language (UML) diagram.\n\n\nIf we look at the UML diagram for the objects in the neotoma2 R package, we can see that there are a set of functions that can operate on sites. As we add to sites objects, using get_datasets() or get_downloads(), we are able to use more of these helper functions. We can use functions like summary() to get a more complete sense of the types of data in this set of sites.\nThe following code gives the summary table. We do some R magic here to change the way the data is displayed (turning it into a datatable() object using the DT package), but the main function is the summary() call.\n\nCodeResult\n\n\n\nneotoma2::summary(mich_sites)\n\n\n\n\n\n\n\n\n\n\n\n\nWe can see that there are no chronologies associated with the site objects. This is because, at present, we have not pulled in the dataset information we need. All we know from get_sites() are the kinds of datasets we have.\n\n\n4.3.1 Searching for Datasets\nNow that we know how to search for sites, we can start to search for datasets. As we’ve discussed before, in the Neotoma data model, each site can contain one or more collection units, each of which can contain one or more datasets. Similarly, a sites object contains collectionunits which contain datasets. From the table above, we can see that some of the sites we’ve looked at contain pollen data. However, so far we have only downloaded the sites data object and not any of the actual pollen data, it’s just that (for convenience) the sites API returns some information about datasets, to make it easier to navigate the records.\nWith a sites object we can directly call get_datasets(), to pull in more metadata about the datasets. At any time we can use datasets() to get more information about any datasets that a sites object may contain. Compare the output of datasets(mich_sites) to the output of a similar call using the following:\n\nCodeResult\n\n\n\nmich_datasets &lt;- neotoma2::get_datasets(mich_sites, all_data = TRUE)\n\ndatasets(mich_datasets)\n\n\n\n\n\nWarning in get_datasets.sites(mich_sites, all_data = TRUE): SiteID 23324, 24000, 24001, 24002, 24003, 24004, 24005, 24006, 24007, 24008, 27284, 27285, 27286, 27287, 27288, 27289, 27317, 27346, 27505, 27506 or DatasetID NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA does not exist in the Neotoma DB yet or it has been removed. \n                        It will be removed from your search.\n\n\n\n\n\n\n\n\n\n\n\n4.3.2 Filter Records\nIf we choose to pull in information about only a single dataset type, or if there is additional filtering we want to do before we download the data, we can use the filter() function. For example, if we only want pollen records, and want records with known chronologies, we can filter:\n\nCodeResult\n\n\n\nmich_pollen &lt;- mich_datasets %&gt;%\n  neotoma2::filter(datasettype == \"pollen\" & !is.na(age_range_young))\n\nneotoma2::summary(mich_pollen)\n\n\n\n\n\n\n\n\n\n\n\n\nNote, that we are filtering on two conditions. (You may want to look up the operators being used in the above code: ==, &, and ! to understand what they accomplish in the code.) We can see now that the data table looks different, and there are fewer total sites.\n\n\n4.3.3 Retrieving sample() data.\nThe sample data are the actual data that scientists usually want - counts of pollen grains, lists of vertebrate fossil occurrences, etc. Because sample data can have fairly large data volumes (each dataset may contain many samples), which can strain server bandwidth and local computing memory, we try to call get_downloads() after we’ve done our preliminary filtering. After get_datasets(), you have enough information to filter based on location, time bounds, and dataset type. When we move to get_downloads() we can do more fine-tuned filtering at the analysis unit or taxon level.\nThe following command may take a few moments to run. (If it takes too long, we have stored an already-downloaded version of the function output as an RDS data file that you can load directly into R.)\n\nmich_dl &lt;- mich_pollen %&gt;% get_downloads(all_data = TRUE)\n\n............................\n\n# mich_dl &lt;- readRDS('data/mich_dl.rds')\n\nOnce we’ve downloaded the sample data, we now have information for each site about all the associated collection units, the datasets, and, for each dataset, all the samples associated with the datasets. To extract all the samples we can call:\n\nallSamp &lt;- samples(mich_dl)\n\nWhen we’ve done this, we get a data.frame that is 40084 rows long and 37 columns wide. The reason the table is so wide is that we are returning data in a long format. Each row contains all the information you should need to properly interpret it:\n\n\n [1] \"age\"             \"agetype\"         \"ageolder\"        \"ageyounger\"     \n [5] \"chronologyid\"    \"chronologyname\"  \"units\"           \"value\"          \n [9] \"context\"         \"element\"         \"taxonid\"         \"symmetry\"       \n[13] \"taxongroup\"      \"elementtype\"     \"variablename\"    \"ecologicalgroup\"\n[17] \"analysisunitid\"  \"sampleanalyst\"   \"sampleid\"        \"depth\"          \n[21] \"thickness\"       \"samplename\"      \"datasetid\"       \"database\"       \n[25] \"datasettype\"     \"age_range_old\"   \"age_range_young\" \"datasetnotes\"   \n[29] \"siteid\"          \"sitename\"        \"lat\"             \"long\"           \n[33] \"area\"            \"sitenotes\"       \"description\"     \"elev\"           \n[37] \"collunitid\"     \n\n\nFor some dataset types, or analyses some of these columns may not be needed, however, for other dataset types they may be critically important. To allow the neotoma2 package to be as useful as possible for the community we’ve included as many as we can.\n\n4.3.3.1 Extracting Taxa\nIf you want to know what taxa we have in a dataset, you can use the helper function taxa() on the sites object. The taxa() function gives us, not only the unique taxa, but two additional columns, sites and samples, that tell us how many sites the taxa appear in, and how many samples the taxa appear in, to help us better understand how common individual taxa are.\n\nCodeResults\n\n\n\nneotomatx &lt;- neotoma2::taxa(mich_dl)\n\n\n\n\n\n\n\n\n\n\n\n\nThe taxonid values can be linked to the taxonid column in the samples(). This allows us to build taxon harmonization tables if we choose to. Note also that the taxonname is in the field variablename. Individual sample counts are reported in Neotoma as variables. A “variable” may be either a species for which we have presence or count data, a geochemical measurement, or any other proxy, such as charcoal counts. Each stored entry for a variable includes the units of measurement and the value.\n\n\n4.3.3.2 Taxonomic Harmonization (Simple)\nA standard challenge in Neotoma (and in biodiversity research more generally) is that different scientists use different names for taxonomic entities such as species. Even if everyone agrees on a common taxonomy, it’s quite possible that a given fossil might be only partially identifiable, perhaps just to genus or even family. Hence, when working with data from Neotoma, a common intermediary step is to ‘harmonize’ all the taxa names stored in Neotoma into some standard names of interest to you.\nLet’s say we want to know the past distribution of Pinus. We want all the various pollen morphotypes that are associated with Pinus (e.g. Pinus strobus, Pinus strobus-type, Pinus undif., Pinus banksiana/resinosa) to be grouped together into one aggregated taxon names called Pinus. There are several ways of doing this, either directly by exporting the file and editing each individual cell, or by creating an external “harmonization” table.\nProgrammatically, we can harmonize all the taxon names using matching and transformation. We’re using dplyr type coding here to mutate() the column variablename so that any time we detect (str_detect()) a variablename that starts with Pinus (the .* represents a wildcard for any character [.], zero or more times [*]) we replace() it with the character string \"Pinus\". Note that this changes Pinus in the allSamp object, but if we were to call samples() again, the taxonomy would return to its original form.\nAs a first step, we’re going to filter the ecological groups to include only UPHE (upleand/heath) and TRSH (trees and shrubs). (More information about ecological groups is available from the Neotoma Online Manual.) After converting all _Pinus._ records to Pinus* we then sum the counts of the Pinus records.\n\nallSamp &lt;- allSamp %&gt;%\n  dplyr::filter(ecologicalgroup %in% c(\"UPHE\", \"TRSH\")) %&gt;%\n  mutate(variablename = replace(variablename,\n                                stringr::str_detect(variablename, \"Pinus.*\"),\n                                \"Pinus\"),\n         variablename = replace(variablename,\n                                stringr::str_detect(variablename, \"Ambrosia.*\"),\n                                \"Ambrosia\")) %&gt;%\n  group_by(siteid, sitename,\n           sampleid, variablename, units, age,\n           agetype, depth, datasetid,\n           long, lat) %&gt;%\n  summarise(value = sum(value), .groups='keep')\n\nThere were originally 6 different taxa identified as being within the genus Pinus (including Pinus, Pinus subg. Pinus, and Pinus undiff.). The above code reduces them all to a single taxonomic group Pinus. We can check out the unique names by using:\n\nCodeResult\n\n\n\nneotomatx %&gt;%\n  ungroup() %&gt;%\n  dplyr::filter(stringr::str_detect(variablename, \"Pinus\")) %&gt;%\n  reframe(pinus_spp = unique(variablename))\n\n# I actually like Base here for the one-liner:\n# unique(grep(\"Pinus\", neotomatx$variablename, value = TRUE))\n\n\n\n\n\n\n\n\n\n\n\n\nIf we want to store a record of our choices outside of R, we can use an external table. For example, a table of pairs (what we want changed, and the name we want it replaced with) can be generated, and it can include regular expressions (if we choose):\n\n\n\noriginal\nreplacement\n\n\n\n\nAbies.*\nAbies\n\n\nVaccinium.*\nEricaceae\n\n\nTypha.*\nAquatic\n\n\nNymphaea\nAquatic\n\n\n…\n…\n\n\n\nWe can get the list of original names directly from the taxa() call, applied to a sites object, and then export it using write.csv(). We can also do some exploratory plots of the data:\n\nCodeResult\n\n\n\ntaxaplots &lt;- taxa(mich_dl)\n# Save the taxon list to file so we can edit it subsequently.\nreadr::write_csv(taxaplots, \"data/mytaxontable.csv\")\n\n\n\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nFigure. A plot of the number of sites a taxon appears in, against the number of samples a taxon appears in.\n\n\n\n\n\n\n\nThe plot is mostly for illustration, but we can see, as a sanity check, that the relationship is as we’d expect.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>The `neotoma2` package</span>"
    ]
  },
  {
    "objectID": "04-neotoma2.html#simple-analytics",
    "href": "04-neotoma2.html#simple-analytics",
    "title": "4  The neotoma2 package",
    "section": "4.5 Simple Analytics",
    "text": "4.5 Simple Analytics\n\n4.5.1 Stratigraphic Plotting: Building a Pollen Diagram\nAs you’ve seen already, stratigraphic diagrams are a very common way of viewing geological data, in which time is represented vertically and with older materials at bottom, just like in the sediment record. Palynologists use a particular form of a stratigraphic diagram called a pollen diagram.\nWe can use packages like rioja to do stratigraphic plotting for a single dataset. Here, we’ll take a few key species at a single site and plot them.\n\n# Get a particular site, select only taxa identified from pollen (and only trees/shrubs)\n# Transform to proportion values.\ndevils_samples &lt;- get_sites(siteid = 666) %&gt;%\n  get_downloads() %&gt;%\n  samples()\n\ndevils_samples &lt;- devils_samples %&gt;%\n  mutate(variablename = replace(variablename,\n                                stringr::str_detect(variablename, \"Pinus.*\"),\n                                \"Pinus\")) %&gt;%\n  group_by(siteid, sitename,\n           sampleid, variablename, units, age,\n           agetype, depth, datasetid,\n           long, lat) %&gt;%\n  summarise(value = sum(value), .groups='keep')\n\n\nonesite &lt;- devils_samples %&gt;%\n  group_by(age) %&gt;%\n  mutate(pollencount = sum(value, na.rm = TRUE)) %&gt;%\n  group_by(variablename) %&gt;%\n  mutate(prop = value / pollencount) %&gt;%\n  arrange(desc(age))\n\n# Spread the data to a \"wide\" table, with taxa as column headings.\nwidetable &lt;- onesite %&gt;%\n  dplyr::select(age, variablename, prop) %&gt;%\n  mutate(prop = as.numeric(prop))  %&gt;%\n  dplyr::filter(variablename %in% c(\"Pinus\", \"Betula\", \"Quercus\",\n                             \"Tsuga\", \"Ulmus\", \"Picea\"))\n\nprops &lt;- tidyr::pivot_wider(widetable,\n                             id_cols = age,\n                             names_from = variablename,\n                             values_from = prop,\n                             values_fill = 0)\n\nThis appears to be a fairly long set of commands, but the code is pretty straightforward, and it provides you with significant control over the taxa for display, units pf measurement, and other elements of your data before you get them into the wide matrix (depth by taxon) that most statistical tools such as the vegan package or rioja use. To plot we can use rioja’s strat.plot(), sorting the taxa using weighted averaging scores (wa.order). We’ve also added a CONISS plot to the edge of the plot, to show how the new wide data frame works with distance metric functions. (We’ll talk more about distance and dissimilarity metrics in upcoming labs.)\n\nclust &lt;- rioja::chclust(dist(sqrt(props)),\n                        method = \"coniss\")\n\nplot &lt;- rioja::strat.plot(props[,-1] * 100, yvar = props$age,\n                  title = devils_samples$sitename[1],\n                  ylabel = \"Calibrated Years BP\",\n                  #xlabel = \"Pollen (%)\",\n                  y.rev = TRUE,\n                  clust = clust,\n                  wa.order = \"topleft\", scale.percent = TRUE)\n\nrioja::addClustZone(plot, clust, 4, col = \"red\")\n\n\n\n\n\n\n\n\nExercise question 6: Make a stratigraphic pollen diagram in rioja, for a site of your choice (not Devils Lake) and taxa of your choice. Show code and resulting diagram.\n\n\n4.5.2 Change Taxon Distributions Across Space and Time\nThe true power of Neotoma is its ability to support large-scale analyses across many sites, many s time periods within sites, many proxies, and many taxa. As a first dipping of our toes in the water, lets look at temporal trends in abundance when averaged across ites. We now have site information across Michigan, with samples, and with taxon names. Let’s say we are interested in looking at the distributions of the selected taxa across time, their presence/absence:\n\ntaxabyage &lt;- allSamp %&gt;%\n  dplyr:::filter(variablename %in% c(\"Pinus\", \"Betula\", \"Quercus\",\n                             \"Tsuga\", \"Ulmus\", \"Picea\"),\n                             age &lt; 11000) %&gt;%\n  group_by(variablename, \"age\" = round(age * 2, -3) / 2) %&gt;%\n  summarise(n = length(unique(siteid)), .groups = 'keep')\n\nsamplesbyage &lt;- allSamp %&gt;%\n  dplyr::filter(variablename %in% c(\"Pinus\", \"Betula\", \"Quercus\",\n                             \"Tsuga\", \"Ulmus\", \"Picea\")) %&gt;%\n  group_by(\"age\" = round(age * 2, -3) / 2) %&gt;%\n  summarise(samples = length(unique(siteid)), .groups = 'keep')\n\ngroupbyage &lt;- taxabyage %&gt;%\n  inner_join(samplesbyage, by = \"age\") %&gt;%\n  mutate(proportion = n / samples)\n\nggplot(groupbyage, aes(x = age, y = proportion)) +\n  geom_point() +\n  geom_smooth(method = 'gam',\n              method.args = list(family = 'binomial')) +\n  facet_wrap(~variablename) +\n  #coord_cartesian(xlim = c(22500, 0), ylim = c(0, 1)) +\n  scale_x_reverse() +\n  xlab(\"Proportion of Sites with Taxon\") +\n  theme_bw()\n\n\n\n\n\n\n\n\nWe can see clear patterns of change for at least some taxa, and the smoothed surfaces are modeled using Generalized Additive Models (GAMs) in R, so we can have more or less control over the actual modeling using the gam or mgcv packages. Depending on how we divide the data we can also look at shifts in altitude, latitude or longitude to better understand how species distributions and abundances changed over time in this region.\nNote that for some taxa, they always have a few pollen grains in all pollen samples, so this ‘proportion of sites with taxon’ isn’t very informative. Calculating a metric like average abundance might be more useful.\nExercise question 7: Repeat the above example, for a different state or other geographic region of your choice.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>The `neotoma2` package</span>"
    ]
  },
  {
    "objectID": "04-neotoma2.html#conclusion",
    "href": "04-neotoma2.html#conclusion",
    "title": "4  The neotoma2 package",
    "section": "4.4 Conclusion",
    "text": "4.4 Conclusion\nSo, we’ve done a lot in this exercise. We’ve (1) learned how APIs work (2) searched for sites using site names and geographic parameters, (3) filtered results using temporal and spatial parameters, (4) built a pollen diagram, and (5) done a first-pass spatial mapping of taxa. We will build upon this methodological foundation in future lab exercises.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>The `neotoma2` package</span>"
    ]
  },
  {
    "objectID": "04-neotoma2.html#to-do",
    "href": "04-neotoma2.html#to-do",
    "title": "4  The neotoma2 package",
    "section": "4.6 To do:",
    "text": "4.6 To do:\n\nCheck ‘Parts’ Should be consistent with other sections\nExercises currently in-text. should be own part like age-depth-modelling?\nerror in all_data = TRUE\ninclude resources section\n\n\n\n\n\nGoring, Simon, Andria Dawson, Gavin Simpson, Karthik Ram, Russ Graham, Eric Grimm, and John Williams. 2015. “Neotoma: A Programmatic Interface to the Neotoma Paleoecological Database.” Open Quaternary 1 (1): Art. 2. https://doi.org/10.5334/oq.ab.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>The `neotoma2` package</span>"
    ]
  },
  {
    "objectID": "05-wrangling-visualisation.html",
    "href": "05-wrangling-visualisation.html",
    "title": "5  Data wrangling and visualisations",
    "section": "",
    "text": "5.1 Simple Analytics",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data wrangling and visualisations</span>"
    ]
  },
  {
    "objectID": "07-no-analogue.html",
    "href": "07-no-analogue.html",
    "title": "7  Novelty and No Analogue Futures",
    "section": "",
    "text": "7.1 Part 1: Background\nThe modern analog technique (MAT) is a standard approach for making quantitative inferences about past vegetation and climate from fossil pollen data – and from many other forms of paleoecological data (Chevalier et al. 2020; J. W. Williams and Shuman 2008; Gavin et al. 2003; Overpeck, Webb, and Prentice 1985). The MAT relies on reasoning by analogy (Jackson and Williams 2004), in which it is assumed that two compositionally similar pollen assemblages were produced by similar plant communities, growing under similar environments. The similarity of pollen assemblages is determined by the calculation of dissimilarity metrics.\nThe MAT can also be thought of as a non-parametric statistical technique, that computer scientists call a \\(k\\)-nearest neighbors algorithm. It is a simple form of machine learning. Each fossil pollen assemblage is matched to 1 or more (\\(k\\)) modern pollen assemblages, then is assigned the ecological and environmental characteristics associated with those modern analogues.\nThe MAT is a popular approach for reconstructing past environments and climates, due to its relative simplicity and intuitive appeal. However, like any algorithm, if used unwisely, it can produce spurious results. More generally, when reasoning by analogy, be careful! Analogies are always incomplete and imperfect, and you must use your critical judgment to determine whether these imperfections are minor or serious.\nTo reconstruct past environments using the MAT, we need three data sets:\nThe MAT follows four basic steps:\nNote that we are taking a detour into paleoclimatology for two reasons. First, because paleoclimatic reconstructions are still a primary use for fossil pollen and other micropaleontological data. Second, because the MAT and the analogue package, as a dissimilarity-based approach, also lets us explore the novelty of past communities - perhaps of more interest to paleoecologists than inferred paleoclimates!\nThe following section, Section 7.2.1, takes you through some pretty advanced data wrangling required to match species names between the datasets we will use. However, if you prefer to skip all the wrangling and jump directly to the anayses in Section 7.2.2, the data have been saved post-wrangling and can be loaded into R directly. Run the ‘Jump point’ code below and portal to Section 7.2.2.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Novelty and No Analogue Futures</span>"
    ]
  },
  {
    "objectID": "07-no-analogue.html#sec-background",
    "href": "07-no-analogue.html#sec-background",
    "title": "7  Novelty and No Analogue Futures",
    "section": "",
    "text": "Modern species assemblages.\nModern environmental variables.\nFossil species assemblages.\n\n\n\nCalculate dissimilarities between a fossil species assemblage \\(i\\) and all modern species assemblages in set \\(S\\).\nDiscard modern samples with dissimilarities greater than a threshold threshold.\nIdentify and retain the \\(k\\) closest modern analogs.\nAverage the environmental conditions associated with the modern analogs and assign them to fossil sample \\(i\\).\n\n\n\n\n\n\n\n\n\nJump point\n\n\n\nRun this code (unfold the code chunk below) to load in the pre-wranged data and portal to Section 7.2.2\n\n\nCode\nfile_names &lt;- c(\"./data/mod_pol_east_match_prop.rds\", \"./data/mod_clim_filter.rds\", \"./data/devils_prop.rds\", \"./data/devils_ages.rds\")\nobj_names &lt;- c(\"mod_pol_east_match_prop\", \"mod_clim_filter\", \"devils_prop\", \"devils_ages\")\nrds_list &lt;- lapply(file_names, readRDS)\nnames(rds_list) &lt;- obj_names\nlist2env(rds_list, globalenv())",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Novelty and No Analogue Futures</span>"
    ]
  },
  {
    "objectID": "07-no-analogue.html#part-2-analysis",
    "href": "07-no-analogue.html#part-2-analysis",
    "title": "7  Novelty and No Analogue Futures",
    "section": "7.2 Part 2: analysis",
    "text": "7.2 Part 2: analysis\n\n7.2.1 Data wrangling\nIn this case, data are coming from two separate sources, NeotomaDB and the North American Modern Pollen Dataset (NAMPD). We will use the NAMPD (Whitmore et al. 2005) for the cross-validation analysis. Note that the NAMPD is pre-loaded into the analogue package, but here we’ll use use the original (more complete) data so that we can filter locations easily. Some of the data wrangling required to get two matrices containing the same species with identical names for the correct region (as required by the MAT function) is fairly advanced. This type of wrangling is quite common in palaeoecological data-science as you may be dealing with, for example, raw data collected by individuals, or from different databases that are stored in inconsistent formats.\nReady to Wrange? Let’s get the data ready:\n\n\n\n\n\n\nPackages required for this section\n\n\n\n\n# Load up the packages\n\nif (!require(\"pacman\")) install.packages(\"pacman\", repos=\"http://cran.r-project.org\")\npacman::p_load(neotoma2, tidyverse, neotoma2, analogue, vegan, readxl, fuzzyjoin)\n\n\n\n\n\n\n\n\n\nRespawn point\n\n\n\nThe following code (unfold by clicking the dropdown) downloads the data from the neotoma API and formats it with basic harmonisation and pollen proportions calculated. This part of the data wrangling is covered in detail in Chapter 4 so, more easily, you can read in the pre-wrangled .rds file from the data directory. Note that this is the long format data not the species only matrix from the jump point above.\n\n\nCode\n# The following code can be done in one pipeline but we have split it into chunks to help with readablity\ndevils_samples &lt;- get_sites(siteid = 666) %&gt;% # Download the site samples data\n  get_downloads() %&gt;%\n  samples()\n\ndevils_samples &lt;- devils_samples %&gt;%\n  dplyr::filter(ecologicalgroup %in% c(\"UPHE\", \"TRSH\"), # Filter ecological groups by upland heath and trees and shrubs\n                elementtype == \"pollen\", # filter bu pollen samples\n                units == \"NISP\") %&gt;% # Filter by sampling unit\n         mutate(variablename = replace(variablename,\n                                stringr::str_detect(variablename, \"Pinus.*\"), # Harmonize Pinus into one group\n                                \"Pinus\"),\n                variablename = replace(variablename,\n                                stringr::str_detect(variablename, \"Ambrosia.*\"),\n                                \"Ambrosia\")) %&gt;%\n  group_by(siteid, sitename,\n           sampleid, variablename, units, age,\n           agetype, depth, datasetid,\n           long, lat) %&gt;%\n  summarise(value = sum(value), .groups='keep') # The group_by function will drop columns not used as grouping variables\n\ndevils_samples &lt;- devils_samples %&gt;% # Calculate proportions of each species by year group\n  group_by(age) %&gt;%\n  mutate(pollencount = sum(value, na.rm = TRUE)) %&gt;%\n  group_by(variablename) %&gt;%\n  mutate(prop = value / pollencount) %&gt;%\n  arrange(desc(age)) %&gt;%\n  ungroup()\n\n# saveRDS(devils_samples, \"./data/devils_samples.rds\") # Save the data for later\n\n\n\n\nTo read in the pre-wrangled long data for Devil’s Lake use:\n\n# Load up the data\ndevils_samples &lt;- readRDS(\"./data/devils_samples.rds\")\n\nWe will use the North American Modern Pollen Database (NAMPD) to create modern analogues. The NAMPD includes both modern surface pollen samples, and climate measurements. The NAMPD is not (yet) accessible through neotoma but is stored on servers at the University of Madison, WI. JACK ACTION POINT: to use this method in other regions what databases are available? The following code does not need to be run but shows how to download the NAMPD from the server to your data directory and unzip the downloaded data from R.\n\ndownload.file(\"https://williamspaleolab.github.io/data/whitmoreetal2005.zip\", \"./data/whitmoreetal2005.zip\")\nunzip(\"./data/whitmoreetal2005.zip\", exdir = \"./data/whitmoreetal2005\")\n\nSince the data are already saved in the data directory, let’s read it in from there. The step of downloading data might often be done manually, the point of including the R code is to maintain a reproducible work-flow. It is always good to keep your own copy of the data just in case. Servers may not be maintained forever, or data may be removed for example.\n\nmod_pol &lt;- read_excel(\"./data/whitmoreetal2005/whitmoreetal2005_v1-72.xls\", sheet = \"POLLEN DATA\")\nmod_clim &lt;- read_excel(\"./data/whitmoreetal2005/whitmoreetal2005_v1-72.xls\", sheet = \"CLIMATE+BIOCLIMATE\")\n\nStill with me? Great! Now we have our three datasets:\n\nOur site of interest (Devil’s Lake palaeo data).\nOur modern pollen data.\nOur modern climate measurements.\n\nThe modern datasets includes the whole of North America and need to be filtered for the Eastern US so that the taxa are relevant to the those in Devil’s Lake. We’re first going to filter by longitude, then select by taxa so that we have a matching matrix of species between Devil’s Lake palaeo-data and the modern surface samples from the NAMPD. We are going to keep all data East of 105°W, see Krause et al. (2019) and John W. Williams, Shuman, and Webb III (2001).\n\n# several columns contain data not used for the MAT\nmod_pol_east &lt;- mod_pol %&gt;% \n  filter(LONDD &gt;= -105) %&gt;% # Filter for sites east of -105 degrees\n  select(ID2, 14:147) # Select colums with ID and species only\n\n# Many functions require site-by-species matrices in wide format\n# Note we pivoted by age so we have an age column not used in MAT\ndevils_wide &lt;- devils_samples %&gt;% \n  pivot_wider(id_cols = age, names_from = variablename, values_from = prop) %&gt;% \n  replace(is.na(.), 0)\n\n# Store ages separately for plotting later:\ndevils_ages &lt;- devils_wide$age\n\nWe will also filter the modern climate records by the same ID to match the modern pollen records.\n\nmod_clim_filter &lt;- mod_clim %&gt;% \n  filter(ID2 %in% mod_pol_east$ID2)\n\nRight, now is the tricky part of matching species names. Unfortunately, the species names between data downloaded from Neotoma and the NAMPD do not match exactly. Sometimes matching is easiest done manually but because of the number of species in both datasets we will do as much as possible programatically. The fuzzyjoin package allows us to match strings (names) by a distance measure and outputs a number of possible matches.\n\nString matching codeLet’s take a closer lookEven closer!\n\n\nlooks like it’s done a good job for the most part! Butthere are a few mismatches.\n\n# Save the names of each dataset and convert all to lowercase\nmod_pol_names &lt;- as_tibble(tolower(colnames(mod_pol_east[-1])))\ndevil_names &lt;- as_tibble(tolower(colnames(devils_wide[-1])))\n\n# match the names using fuzzy matching\nnames_matches &lt;- stringdist_left_join(devil_names, mod_pol_names, by = \"value\",\n                                      max_dist = 4, distance_col = \"distance\", method = \"jw\") %&gt;% \n                   group_by(value.x) %&gt;%\n                   slice_min(order_by = distance, n = 1) # change the value of n to see more potential matches\n\nhead(names_matches, 15) \n\n# A tibble: 15 × 3\n# Groups:   value.x [15]\n   value.x                      value.y    distance\n   &lt;chr&gt;                        &lt;chr&gt;         &lt;dbl&gt;\n 1 abies                        abies        0     \n 2 acer                         acerx        0.0667\n 3 alnus                        alnusx       0.0556\n 4 amaranthaceae undiff.        rhamnaceae   0.308 \n 5 ambrosia                     ambrosia     0     \n 6 amorpha-type                 amorpha      0.139 \n 7 angiospermae undiff. (herbs) poaceae      0.365 \n 8 apiaceae                     apiaceae     0     \n 9 artemisia                    artemisia    0     \n10 asteraceae undiff.           asterx       0.296 \n11 betula                       betula       0     \n12 cannabaceae sensu stricto    abies        0.347 \n13 caprifoliaceae sensu lato    caprifolia   0.2   \n14 carya                        carya        0     \n15 castanea                     castanea     0     \n\n\n\n\nIn your console take a good look through the whole list. You don’t want anything matched twice!\n\nnames_matches %&gt;% filter(distance &gt; 0.1) # looks like everything below 0.1 is a good match\n\n# A tibble: 20 × 3\n# Groups:   value.x [19]\n   value.x                      value.y    distance\n   &lt;chr&gt;                        &lt;chr&gt;         &lt;dbl&gt;\n 1 amaranthaceae undiff.        rhamnaceae    0.308\n 2 amorpha-type                 amorpha       0.139\n 3 angiospermae undiff. (herbs) poaceae       0.365\n 4 asteraceae undiff.           asterx        0.296\n 5 cannabaceae sensu stricto    abies         0.347\n 6 caprifoliaceae sensu lato    caprifolia    0.2  \n 7 chenopodium-type             chenoamx      0.292\n 8 cupressaceae                 cupressa      0.111\n 9 ericaceae                    arecaceae     0.116\n10 ericaceae                    ericacex      0.116\n11 fabaceae undiff.             fabaceae      0.167\n12 ilex                         piceax        0.25 \n13 larix                        larixpseu     0.148\n14 morus                        cornus        0.178\n15 ostrya/carpinus              ostrycar      0.197\n16 plantago                     plantaginx    0.142\n17 rhamnaceae/vitaceae          rhamnaceae    0.158\n18 rhus                         rubus         0.217\n19 sarcobatus vermiculatus      sarcobatus    0.188\n20 urticaceae                   urticacx      0.142\n\n\n\n\nArranging by distance helps.\n\nnames_matches %&gt;%\n  filter(distance &lt; 0.3)  %&gt;% # everything above 0.3 looks like a bad match\n  arrange(desc(distance)) %&gt;% \n  print(n = 15)\n\n# A tibble: 49 × 3\n# Groups:   value.x [48]\n   value.x                   value.y    distance\n   &lt;chr&gt;                     &lt;chr&gt;         &lt;dbl&gt;\n 1 asteraceae undiff.        asterx        0.296\n 2 chenopodium-type          chenoamx      0.292\n 3 ilex                      piceax        0.25 \n 4 rhus                      rubus         0.217\n 5 caprifoliaceae sensu lato caprifolia    0.2  \n 6 ostrya/carpinus           ostrycar      0.197\n 7 sarcobatus vermiculatus   sarcobatus    0.188\n 8 morus                     cornus        0.178\n 9 fabaceae undiff.          fabaceae      0.167\n10 rhamnaceae/vitaceae       rhamnaceae    0.158\n11 larix                     larixpseu     0.148\n12 plantago                  plantaginx    0.142\n13 urticaceae                urticacx      0.142\n14 amorpha-type              amorpha       0.139\n15 ericaceae                 arecaceae     0.116\n# ℹ 34 more rows\n\n\n\n\n\nOk looks like we only have a few to weed out manually:\n\nnames_matches_filter &lt;- names_matches %&gt;% \n  filter(distance &lt; 0.3,\n         value.x != \"ilex\" & value.y != \"piceax\",\n         value.x != \"rhus\" & value.y != \"rubus\",\n         value.x != \"morus\" & value.y != \"cornus\",\n         value.x != \"ericaceae\" & value.y != \"arecaceae\")  %&gt;% \n         arrange(desc(distance))\n\nNow we have a dataframe (names_matches_filter) with names from Devil’s Lake and their match in the NAMPD. There are two routes for this analysis:\n\nJoin the two datasets so that both contain the same spicies column names, creating two larger dataset. Species missing from one dataset are filled with zero.\nConduct the analysis on the intersect of the two datasets, i.e., a smaller dataset of the species common between the two.\n\nNow that we have matched the names between the two datasets, it is not difficult to accomplish either method. The datasets can be expanded so that both contain all the same species using the join function in the analogue package. Conversely, the datasets can be shrunk by selecting the species common to both using the names filter. The code for running the MAT for both methods is the same, so we will create datasets for both methods. We will use the first, more conventional, method for the rest of the chapter and, if you want to explore the second method, you can replace the data in the code with the ‘Intersec’ data.\n\nJoinIntersect\n\n\nThis code joins the modern pollen data and the Devil’s Lake core data so that the number of columns is expanded to contain the same species in both. Note that while the variables (columns and species names) have been matched, each dataset remains independent. The two datasets have not been merged into one dataset like the *_join functions from tidyr.\n\nmod_pol_east_join &lt;- mod_pol_east %&gt;%\n  rename_with(tolower) %&gt;% \n  rename(all_of(deframe(names_matches_filter[ ,1:2]))) %&gt;% \n  select(-id2) %&gt;% # We do not want the ID column in the calculation\n  mutate(across(everything(), ~ replace_na(.x, 0))) # Data contain some NA values that we can safely assume to be a zero-count\nmod_pol_east_join &lt;- mod_pol_east_join / rowSums(mod_pol_east_join) # I prefer the BASE R way of doing this calculation\n# rowSums(mod_pol_east_join) # I regularly use rowSums on wide data as a data check.\n# Prevents accidentally using counts or percentages instead of proportions, or including age/depth/ID columns\n\ndevils_prop &lt;- devils_wide %&gt;% \n  rename_with(tolower) %&gt;% \n  select(-age)\n# rowSums(devils_prop) # Data check\n\njoin_dat &lt;- join(mod_pol_east_join, devils_prop, verbose = TRUE)\n\n\nSummary:\n\n            Rows Cols\nData set 1: 2594  134\nData set 2:  123   51\nMerged:     2717  142\n\nmod_pol_east_join &lt;- join_dat$mod_pol_east_join\ndevils_prop &lt;- join_dat$devils_prop\n\n\n\nThis code selects the common species of both the modern pollen data and the Devil’s Lake core data and reduces the number of columns of both.\n\n# Select species based on name\nmod_pol_east_intersect &lt;- mod_pol_east %&gt;%\n  rename_with(tolower) %&gt;% # convert to lowercase for matching\n  select(names_matches_filter$value.y) %&gt;%  # keep id column and select species\n  as.matrix()\n\n# The analoge functions need species names to match between datasets\n# so lets name everything in our filtered modern pollen the same as in Devil's Lake\nmod_pol_east_intersect &lt;- mod_pol_east_intersect %&gt;% \n  rename(all_of(deframe(names_matches_filter[ ,1:2])))\n\n# Let's create a matrix of only species for the MAT with relative proportions\n# This is an approach for calculating relative abundances on wide data:\nmod_pol_east_prop_intersect &lt;- mod_pol_east_intersect / rowSums(mod_pol_east_intersect)\n# data check: rowSums(mod_pol_east_prop_intersect)\n\n# while we are here let's make a matrix of species only from Devils Lake\ndevils_prop_intersect &lt;- devils_wide %&gt;% \n  rename_with(tolower) %&gt;% \n  select(names_matches_filter$value.x) %&gt;% # Remember we removed a few species without matches?\n  as.matrix()\n\n# Recall that we already converted the Devil's Lake data to proportions,\n# but, we need to re-cecalculate proportions after dropping columns.\n# data check: rowSums(devils_prop_intersect)\ndevils_prop_intersect &lt;- devils_prop_intersect / rowSums(devils_prop_intersect) \n\n\n\n\n\n\n\n\n\n\nGood practice tip\n\n\n\nWhenever filtering or joining data it is good to check the dimensions before and after by using dim() to see if the results are what you expect. Some functions like left_join() or full_join() may run without error but produce unexpected results. It is also good to regularly use rowSums() to make sure you are using the correct data type (counts, proportions or percentages), and have not accidentaly included age, depth, or ID columns if they should be excluded.\n\n\nThat took some pretty advanced wrangling to get the data in order. Such is often the case when using multiple sources of data. As always, there are many ways to achieve the same result. You can use these data wrangling tricks on different datasets and save a lot of time doing manual work.\n\n\n7.2.2 Cross-validation\nThe Modern Analogue Technique (MAT) needs the same set of species in both the modern dataset and the core data, and can be calculated in two ways (restated here if you skipped the data wrangling in Section 7.2.1):\n\nBy joining the two datasets so that both contain the same spicies column names, creating two larger dataset. Species missing from one dataset are filled with zero.\nBy conducting the analysis on the intersect of the two datasets, i.e., smaller datasets of the species common between the two.\n\nThe remainder of the chapter will use the first, more conventional, method of joining datasets to contain the same species. In Section 7.2.1, we created a dataset for both methods. The code below works for both methods, and you can switch out the data if you want to (create the intersect data by running the code block “intersect” at the end of the previous section).\nBefore reconstructing environmental variables from fossil pollen assemblages, we usually assess how MAT performs in the modern species assemblage. This step is usually referred to as calibration or cross-validation.\nIn cross-validation, a calibration function is trained on one portion of the modern species assemblages (calibration set) and applied to another portion of the modern species assemblages (validation set). These two datasets are thus mutually exclusive and - possibly - independent. To cross-validate MAT, we usually use a technique called \\(k\\)-fold cross-validation. In \\(k\\)-fold cross-validation the modern data set is split into \\(k\\) mutually exclusive subsets. For each \\(k^{th}\\) subset, the calibration dataset comprises all the samples not in \\(k\\), while the samples in \\(k\\) comprise the validation dataset. The simplest form of \\(k\\)-fold cross-validation is the leave-one-out (LOO) technique, in which just a single sample is removed and then all other samples are used to build a prediction for that sample. This procedure is then repeated for all samples. The analogue package uses leave-one-out cross-validation.\nStandard metrics include root mean square error (RMSE) and \\(R^2\\). Here, we’ll use the cross-validation tools built into the analogue package.\nNote that the palaeoSig package, developed by Richard Telford, has additional functions that can test for significance relative to randomly generated variables with the same spatial structure as the environmental variable of interest. We won’t use this package in this lab, but it’s useful for testing whether apparently strong cross-validation results are merely an artifact of spatial autocorrelation (Telford and Birks 2005).\nNow let’s do the analysis! First is the cross validation of the modern datasets. Using the modern data, we’ll build a calibration dataset and run cross-validation analyses of the calibration dataset. If you portaled here and skipped the Data Wrangling in Section 7.2.1, make sure you load in the pre-formatted data from Section 7.1.\n\nCodeSummaryPlots\n\n\n\nmod_jant &lt;- data.matrix(mod_clim_filter$tjan)\n\nmodpoll_jant &lt;- analogue::mat(mod_pol_east_join, mod_jant, method=\"SQchord\")\n\n\n\n\nprint(modpoll_jant)\n\n\n    Modern Analogue Technique\n\nCall:\nmat(x = mod_pol_east_join, y = mod_jant, method = \"SQchord\") \n\nPercentiles of the dissimilarities for the training set:\n\n   1%    2%    5%   10%   20% \n0.207 0.272 0.385 0.502 0.670 \n\nInferences based on the mean of k-closest analogues:\n\n  k  RMSEP     R2 Avg Bias Max Bias\n  1  2.766  0.934   -0.214   -1.625\n  2  2.654  0.939   -0.302   -2.265\n  3  2.537  0.944   -0.360   -2.648\n  4  2.608  0.941   -0.350   -2.569\n  5  2.661  0.938   -0.357   -2.622\n  6  2.714  0.936   -0.361   -2.714\n  7  2.716  0.936   -0.373   -2.804\n  8  2.734  0.935   -0.368   -2.982\n  9  2.779  0.933   -0.357   -3.104\n 10  2.826  0.930   -0.370   -3.316\n\nInferences based on the weighted mean of k-closest analogues:\n\n  k RMSEP    R2 Avg Bias Max Bias\n  1   NaN    NA      NaN    -1.62\n  2   NaN    NA      NaN    -2.19\n  3   NaN    NA      NaN    -2.52\n  4   NaN    NA      NaN    -2.46\n  5   NaN    NA      NaN    -2.52\n  6   NaN    NA      NaN    -2.59\n  7   NaN    NA      NaN    -2.68\n  8   NaN    NA      NaN    -2.81\n  9   NaN    NA      NaN    -2.93\n 10   NaN    NA      NaN    -3.11\n\n\n\n\n\npar(mfrow = c(2,2))\nplot(modpoll_jant)\n\n\n\n\n\n\n\n\n\n\n\nMuch of the information in this chapter comes from Simpson (2007), and we recommend Simpson (2007) for the interpretation of the outputs. The MAT method shares similar considerations as calculating dissimilarity (Chapter 6). With some additional considerations regarding transfer functions:\n\nTime intervals and chronological uncertainties are not accounted for. The difference in time between successive samples often varies in palaeo-data.\nTime averaging is not taken into account, and each sample may contain a different time-span of ecological productivity.\nSquared chord distance is one of many distance measures available in analogue, and may not always be the most appropriate.\n\nFor deeper considerations of reconstructing temperatures using transfer functions see Telford and Birks (2005).\n\n\n7.2.3 Novelty and Paleoclimatic Reconstruction\nJack action point Section needs some info from the lecture content to help understand the process as a stand-alone unit.\nFor this part of the lab, we’ll first assess the novelty of the fossil pollen assemblages at Devil’s Lake. We’ll then reconstruct past climates at Devil’s Lake, using the MAT. Here’s some demonstration code showing the kinds of analyses that can be done and plotted timeseries.\n\nMinimum distanceModern analoguesReconstruction\n\n\nConstruct a time series of the minimum SCDs (novelty) for Devil’s Lake.\n\n# Predict temperatures from the fossil data and their modern analogue\ndevil_jant &lt;- predict(modpoll_jant, devils_prop, k=10)\n\npar(mfrow = c(1,1))\ndevil_mindis &lt;- minDC(devil_jant)\nplot(devil_mindis, depths = devils_ages, quantiles = FALSE, xlab = \"Age (yr BP)\", ylab = \"minSCD\")\n\n\n\n\n\n\n\n\n\n\nConstruct a time series of the number of modern analogs found per fossil sample, using 0.25 as our no-analog/analog threshold.\n\ndevil_cma &lt;- analogue::cma(devil_jant, cutoff=0.25)\n\nplot(x = devils_ages, y = devil_cma$n.analogs, type = \"l\", xlim= rev(range(devils_ages)), xlab = \"Age (yr BP)\", ylab = \"Number of modern analogs\")\n\n\n\n\n\n\n\n\n\n\nPlot the reconstructed January temperatures at Devil’s Lake:\n\nreconPlot(devil_jant, depth = devils_ages, display.error=\"bars\", xlab = \"Age (yr BP)\", ylab = \"Mean Jan temperature\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nResources\n\n\n\nJacktion point:\nCopy in a list of DOIs for essential reading and I’ll add insert them as formatted citation links:\n\nDOI\nDOI",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Novelty and No Analogue Futures</span>"
    ]
  },
  {
    "objectID": "07-no-analogue.html#part-3-exercises",
    "href": "07-no-analogue.html#part-3-exercises",
    "title": "7  Novelty and No Analogue Futures",
    "section": "7.3 Part 3: Exercises",
    "text": "7.3 Part 3: Exercises\nJacktion point: review questions and question order :)\n\nIn the summary output for JanT, which choice of k (number of analogues) produces the highest R2 and lowest RMSEP?\n\nWhich samples at Devil’s Lake lack modern analogs?\n\nRepeat the above analysis for TJul, PJul, GDD5, and MIPT. (GDD5 is growing degree days and is a measure of growing season length and strength; MIPT is an index of soil moisture availability).\nWhich variable has the best cross-validation statistics?\n\nWhich has the worst?\n\nGiven what you know about the climatic controls on plant distributions (and/or our ability to precisely quantify these variables), is the relative predictive skill for these different climatic variables surprising or unsurprising? Why?\nCheck the Devil’s Lake novelty curve against the pollen diagram for Devil’s Lake. What combinations of pollen taxa produce these no-analog assemblages?\nAccording to Webb(1986), what are two general hypotheses for why we might have these past mixtures of species with no modern analogue?\nWhy should one be dubious about the temperature reconstructions at Devil’s Lake for the high-novelty assemblages?\nEven for the low-novelty assemblages, name two potentially important sources of uncertainty in MAT-based reconstructions of past climate from fossil pollen data.\n\nConversely, based on our knowledge about the geographic climatic distributions of these taxa, why might one argue that these January temperature reconstructions are plausible?\n\nTo get practice, make reconstructions of GDD5 and mean July precipitation. Show plots of your work.\n\n\n\n\n\nChevalier, Manuel, Basil A. S. Davis, Oliver Heiri, Heikki Seppä, Brian M. Chase, Konrad Gajewski, Terri Lacourse, et al. 2020. “Pollen-Based Climate Reconstruction Techniques for Late Quaternary Studies.” Earth-Science Reviews 210 (November): 103384. https://doi.org/10.1016/j.earscirev.2020.103384.\n\n\nGavin, Daniel G, W.Wyatt Oswald, Eugene R Wahl, and John W Williams. 2003. “A Statistical Approach to Evaluating Distance Metrics and Analog Assignments for Pollen Records.” Quaternary Research 60 (3): 356–67. https://doi.org/10.1016/S0033-5894(03)00088-7.\n\n\nJackson, Stephen T., and John W. Williams. 2004. “Modern Analogs in Quaternary Paleoecology: Here Today, Gone Yesterday, Gone Tomorrow?” Annual Review of Earth and Planetary Sciences 32 (1): 495–537. https://doi.org/10.1146/annurev.earth.32.101802.120435.\n\n\nKrause, Teresa R., James M. Russell, Rui Zhang, John W. Williams, and Stephen T. Jackson. 2019. “Late Quaternary Vegetation, Climate, and Fire History of the Southeast Atlantic Coastal Plain Based on a 30,000-Yr Multi-Proxy Record from White Pond, South Carolina, USA.” Quaternary Research 91 (2): 861–80. https://doi.org/10.1017/qua.2018.95.\n\n\nOverpeck, J. T., T. Webb, and I. C. Prentice. 1985. “Quantitative Interpretation of Fossil Pollen Spectra: Dissimilarity Coefficients and the Method of Modern Analogs.” Quaternary Research 23 (1): 87–108. https://doi.org/10.1016/0033-5894(85)90074-2.\n\n\nRadeloff, Volker C., John W. Williams, Brooke L. Bateman, Kevin D. Burke, Sarah K. Carter, Evan S. Childress, Kara J. Cromwell, et al. 2015. “The Rise of Novelty in Ecosystems.” Ecological Applications 25 (8): 2051–68. https://doi.org/10.1890/14-1781.1.\n\n\nSimpson, Gavin L. 2007. “Analogue Methods in Palaeoecology: Using the Analogue Package.” Journal of Statistical Software 22 (September): 1–29. https://doi.org/10.18637/jss.v022.i02.\n\n\nTelford, R. J., and H. J. B. Birks. 2005. “The Secret Assumption of Transfer Functions: Problems with Spatial Autocorrelation in Evaluating Model Performance.” Quaternary Science Reviews 24 (20-21): 2173–79. https://doi.org/10.1016/j.quascirev.2005.05.001.\n\n\nWhitmore, J., K. Gajewski, M. Sawada, J. W. Williams, B. Shuman, P. J. Bartlein, T. Minckley, et al. 2005. “Modern Pollen Data from North America and Greenland for Multi-Scale Paleoenvironmental Applications.” Quaternary Science Reviews 24 (16-17): 1828–48. https://doi.org/10.1016/j.quascirev.2005.03.005.\n\n\nWilliams, J. W., and B. Shuman. 2008. “Obtaining Accurate and Precise Environmental Reconstructions from the Modern Analog Technique and North American Surface Pollen Dataset.” Quaternary Science Reviews 27 (7-8): 669–87. https://doi.org/10.1016/j.quascirev.2008.01.004.\n\n\nWilliams, John W., Bryan N. Shuman, and Thompson Webb III. 2001. “Dissimilarity Analyses of Late-Quaternary Vegetation and Climate in Eastern North America.” Ecology 82 (12): 3346–62. https://doi.org/10.1890/0012-9658(2001)082[3346:DAOLQV]2.0.CO;2.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Novelty and No Analogue Futures</span>"
    ]
  },
  {
    "objectID": "08-rate-of-change.html",
    "href": "08-rate-of-change.html",
    "title": "8  Dissimilarity and Rates of Change",
    "section": "",
    "text": "Here is an equation.\n\\[\\begin{equation}\n  f\\left(k\\right) = \\binom{n}{k} p^k\\left(1-p\\right)^{n-k}\n  (\\#eq:binom)\n\\end{equation}\\]",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Dissimilarity and Rates of Change</span>"
    ]
  },
  {
    "objectID": "09-ordination-pca.html",
    "href": "09-ordination-pca.html",
    "title": "9  Ordination: Principal Component Analysis",
    "section": "",
    "text": "Principal Component Analysis (PCA) is a statistical method for reducing \\(n\\)-dimensional data (e.g., the \\(n\\) species in your data) to fewer axes that minimise the variance in the data.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Ordination: Principal Component Analysis</span>"
    ]
  },
  {
    "objectID": "10-ordination-nmds.html",
    "href": "10-ordination-nmds.html",
    "title": "10  Ordination: Non-metric Dimension Scaling",
    "section": "",
    "text": "I need wifi.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Ordination: Non-metric Dimension Scaling</span>"
    ]
  },
  {
    "objectID": "11-gams.html",
    "href": "11-gams.html",
    "title": "11  Generalised Additive Models",
    "section": "",
    "text": "11.1 Hierarchical Generalised Additive Models.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Generalised Additive Models</span>"
    ]
  },
  {
    "objectID": "12-tapas.html",
    "href": "12-tapas.html",
    "title": "12  Charcoal Analysis using TAPAS",
    "section": "",
    "text": "Tapas is a R package for analysing charcoal",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Charcoal Analysis using TAPAS</span>"
    ]
  },
  {
    "objectID": "13-psm.html",
    "href": "13-psm.html",
    "title": "13  Proxy System Modelling",
    "section": "",
    "text": "Should this be replaced with LDA/CTM?",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Proxy System Modelling</span>"
    ]
  },
  {
    "objectID": "15-test-chapter.html",
    "href": "15-test-chapter.html",
    "title": "14  Test pad to delete",
    "section": "",
    "text": "Sandbox chapter for testing scripts outside main writing if necessary\n\n\n\n\n\n\nPackages required for this section\n\n\n\nList of packages required for section and what they are used for\n\n\n\n\n\n\n\n\nRespawn code\n\n\n\nBegin each section with a code-block that can be run to reproduce the necessary data format for the upcoming section\n\n\n\n\n\n\n\n\nOh no! 😱\n\n\n\nExample of common mistakes\n\n\n\n\n\n\n\n\nGood practice tip\n\n\n\nTips on version control, directory structure (and anything else) that make life easier\n\n\n\n\n\n\n\n\nResources\n\n\n\nImportant readings",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Test pad to delete</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Blaauw, Maarten, J. Andrés Christen, K. D. Bennett, and Paula J. Reimer.\n2018. “Double the Dates and Go for Bayes —\nImpacts of Model Choice, Dating Density and Quality on\nChronologies.” Quaternary Science Reviews 188 (May):\n58–66. https://doi.org/10.1016/j.quascirev.2018.03.032.\n\n\nBlei, David M. 2012. “Probabilistic Topic Models.”\nCommunications of the ACM 55 (4): 77–84. https://doi.org/10.1145/2133806.2133826.\n\n\nBlei, David M., and John D. Lafferty. 2007. “A Correlated Topic\nModel of Science.” The Annals of Applied\nStatistics 1 (1). https://doi.org/10.1214/07-AOAS114.\n\n\nBlois, Jessica L., John W. (Jack) Williams, Eric C. Grimm, Stephen T.\nJackson, and Russell W. Graham. 2011. “A Methodological Framework\nfor Assessing and Reducing Temporal Uncertainty in Paleovegetation\nMapping from Late-Quaternary Pollen Records.”\nQuaternary Science Reviews 30 (15-16): 1926–39. https://doi.org/10.1016/j.quascirev.2011.04.017.\n\n\nChevalier, Manuel, Basil A. S. Davis, Oliver Heiri, Heikki Seppä, Brian\nM. Chase, Konrad Gajewski, Terri Lacourse, et al. 2020.\n“Pollen-Based Climate Reconstruction Techniques for Late\nQuaternary Studies.” Earth-Science Reviews\n210 (November): 103384. https://doi.org/10.1016/j.earscirev.2020.103384.\n\n\nElith, J., J. R. Leathwick, and T. Hastie. 2008. “A Working Guide\nto Boosted Regression Trees.” Journal of Animal Ecology\n77 (4): 802–13. https://doi.org/10.1111/j.1365-2656.2008.01390.x.\n\n\nGavin, Daniel G, W.Wyatt Oswald, Eugene R Wahl, and John W Williams.\n2003. “A Statistical Approach to Evaluating Distance Metrics and\nAnalog Assignments for Pollen Records.” Quaternary\nResearch 60 (3): 356–67. https://doi.org/10.1016/S0033-5894(03)00088-7.\n\n\nGoring, Simon, Andria Dawson, Gavin Simpson, Karthik Ram, Russ Graham,\nEric Grimm, and John Williams. 2015. “Neotoma: A\nProgrammatic Interface to the Neotoma Paleoecological\nDatabase.” Open Quaternary 1 (1): Art. 2. https://doi.org/10.5334/oq.ab.\n\n\nGoring, S., J. W. Williams, J. L. Blois, S. T. Jackson, C. J. Paciorek,\nR. K. Booth, J. R. Marlon, M. Blaauw, and J. A. Christen. 2012.\n“Deposition Times in the Northeastern United States\nDuring the Holocene: Establishing Valid Priors for\nBayesian Age Models.” Quaternary Science\nReviews 48 (August): 54–60. https://doi.org/10.1016/j.quascirev.2012.05.019.\n\n\nGrimm, Eric C. 1987. “CONISS: A FORTRAN\n77 Program for Stratigraphically Constrained Cluster Analysis by the\nMethod of Incremental Sum of Squares.” Computers &\nGeosciences 13 (1): 13–35. https://doi.org/10.1016/0098-3004(87)90022-7.\n\n\nJackson, Stephen T., and John W. Williams. 2004. “Modern\nAnalogs in Quaternary Paleoecology: Here\nToday, Gone Yesterday, Gone\nTomorrow?” Annual Review of Earth and Planetary\nSciences 32 (1): 495–537. https://doi.org/10.1146/annurev.earth.32.101802.120435.\n\n\nKrause, Teresa R., James M. Russell, Rui Zhang, John W. Williams, and\nStephen T. Jackson. 2019. “Late Quaternary\nVegetation, Climate, and Fire History of the Southeast Atlantic\nCoastal Plain Based on a 30,000-Yr Multi-Proxy Record from\nWhite Pond, South Carolina,\nUSA.” Quaternary Research 91 (2): 861–80.\nhttps://doi.org/10.1017/qua.2018.95.\n\n\nMottl, Ondřej, Suzette G. A. Flantua, Kuber P. Bhatta, Vivian A. Felde,\nThomas Giesecke, Simon Goring, Eric C. Grimm, et al. 2021. “Global\nAcceleration in Rates of Vegetation Change over the Past 18,000\nYears.” Science 372 (6544): 860–64. https://doi.org/10.1126/science.abg1685.\n\n\nOverpeck, J. T., T. Webb, and I. C. Prentice. 1985. “Quantitative\nInterpretation of Fossil Pollen Spectra:\nDissimilarity Coefficients and the Method of\nModern Analogs.” Quaternary Research 23\n(1): 87–108. https://doi.org/10.1016/0033-5894(85)90074-2.\n\n\nParnell, A. C., J. Haslett, J. R. M. Allen, C. E. Buck, and B. Huntley.\n2008. “A Flexible Approach to Assessing Synchroneity of Past\nEvents Using Bayesian Reconstructions of Sedimentation\nHistory.” Quaternary Science Reviews 27 (19-20):\n1872–85. https://doi.org/10.1016/j.quascirev.2008.07.009.\n\n\nRadeloff, Volker C., John W. Williams, Brooke L. Bateman, Kevin D.\nBurke, Sarah K. Carter, Evan S. Childress, Kara J. Cromwell, et al.\n2015. “The Rise of Novelty in Ecosystems.” Ecological\nApplications 25 (8): 2051–68. https://doi.org/10.1890/14-1781.1.\n\n\nReimer, Paula J, William E N Austin, Edouard Bard, Alex Bayliss, Paul G\nBlackwell, Christopher Bronk Ramsey, Martin Butzin, et al. 2020.\n“The IntCal20 Northern Hemisphere Radiocarbon Age\nCalibration Curve (0–55 Cal kBP).” Radiocarbon 62 (4): 725–57.\nhttps://doi.org/10.1017/RDC.2020.41.\n\n\nSimpson, Gavin L. 2007. “Analogue Methods in\nPalaeoecology: Using the Analogue\nPackage.” Journal of Statistical Software\n22 (September): 1–29. https://doi.org/10.18637/jss.v022.i02.\n\n\nSimpson, Gavin L., and H. John B. Birks. 2012. “Statistical\nLearning in Palaeolimnology.” In Tracking Environmental\nChange Using Lake Sediments: Data Handling and\nNumerical Techniques, edited by H. John B. Birks,\nAndré F. Lotter, Steve Juggins, and John P. Smol, 249–327. Developments\nin Paleoenvironmental Research. Dordrecht: Springer\nNetherlands. https://doi.org/10.1007/978-94-007-2745-8_9.\n\n\nSmith, A. G., and I. C. Goddard. 1991. “A 12500 Year\nRecord of Vegetational History at Sluggan\nBog, Co. Antrim, N.\nIreland (Incorporating a Pollen Zone\nScheme for the Non-Specialist).” The New\nPhytologist 118 (1): 167–87. https://www.jstor.org/stable/2557698.\n\n\nTelford, R. J., and H. J. B. Birks. 2005. “The Secret Assumption\nof Transfer Functions: Problems with Spatial Autocorrelation in\nEvaluating Model Performance.” Quaternary Science\nReviews 24 (20-21): 2173–79. https://doi.org/10.1016/j.quascirev.2005.05.001.\n\n\nTrachsel, Mathias, and Richard J Telford. 2017. “All Age–Depth\nModels Are Wrong, but Are Getting Better.” The Holocene\n27 (6): 860–69. https://doi.org/10.1177/0959683616675939.\n\n\nWhitmore, J., K. Gajewski, M. Sawada, J. W. Williams, B. Shuman, P. J.\nBartlein, T. Minckley, et al. 2005. “Modern Pollen Data from\nNorth America and Greenland for Multi-Scale\nPaleoenvironmental Applications.” Quaternary Science\nReviews 24 (16-17): 1828–48. https://doi.org/10.1016/j.quascirev.2005.03.005.\n\n\nWilliams, J. W., and B. Shuman. 2008. “Obtaining Accurate and\nPrecise Environmental Reconstructions from the Modern Analog Technique\nand North American Surface Pollen Dataset.”\nQuaternary Science Reviews 27 (7-8): 669–87. https://doi.org/10.1016/j.quascirev.2008.01.004.\n\n\nWilliams, John W., Bryan N. Shuman, and Thompson Webb III. 2001.\n“Dissimilarity Analyses of Late-Quaternary\nVegetation and Climate in Eastern North\nAmerica.” Ecology 82 (12): 3346–62. https://doi.org/10.1890/0012-9658(2001)082[3346:DAOLQV]2.0.CO;2.",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "05-wrangling-visualisation.html#simple-analytics",
    "href": "05-wrangling-visualisation.html#simple-analytics",
    "title": "5  Data wrangling and visualisations",
    "section": "",
    "text": "5.1.1 Stratigraphic Plotting: Building a Pollen Diagram\nAs you’ve seen already, stratigraphic diagrams are a very common way of viewing geological data, in which time is represented vertically and with older materials at bottom, just like in the sediment record. Palynologists use a particular form of a stratigraphic diagram called a pollen diagram.\nWe can use packages like rioja to do stratigraphic plotting for a single dataset. Here, we’ll take a few key species at a single site and plot them.\n\n# Get a particular site, select only taxa identified from pollen (and only trees/shrubs)\n# Transform to proportion values.\ndevils_samples &lt;- get_sites(siteid = 666) %&gt;%\n  get_downloads() %&gt;%\n  samples()\n\ndevils_samples &lt;- devils_samples %&gt;%\n  mutate(variablename = replace(variablename,\n                                stringr::str_detect(variablename, \"Pinus.*\"),\n                                \"Pinus\")) %&gt;%\n  group_by(siteid, sitename,\n           sampleid, variablename, units, age,\n           agetype, depth, datasetid,\n           long, lat) %&gt;%\n  summarise(value = sum(value), .groups='keep')\n\n\nonesite &lt;- devils_samples %&gt;%\n  group_by(age) %&gt;%\n  mutate(pollencount = sum(value, na.rm = TRUE)) %&gt;%\n  group_by(variablename) %&gt;%\n  mutate(prop = value / pollencount) %&gt;%\n  arrange(desc(age))\n\n# Spread the data to a \"wide\" table, with taxa as column headings.\nwidetable &lt;- onesite %&gt;%\n  dplyr::select(age, variablename, prop) %&gt;%\n  mutate(prop = as.numeric(prop))  %&gt;%\n  dplyr::filter(variablename %in% c(\"Pinus\", \"Betula\", \"Quercus\",\n                             \"Tsuga\", \"Ulmus\", \"Picea\"))\n\nprops &lt;- tidyr::pivot_wider(widetable,\n                             id_cols = age,\n                             names_from = variablename,\n                             values_from = prop,\n                             values_fill = 0)\n\nThis appears to be a fairly long set of commands, but the code is pretty straightforward, and it provides you with significant control over the taxa for display, units pf measurement, and other elements of your data before you get them into the wide matrix (depth by taxon) that most statistical tools such as the vegan package or rioja use. To plot we can use rioja’s strat.plot(), sorting the taxa using weighted averaging scores (wa.order). We’ve also added a CONISS plot to the edge of the plot, to show how the new wide data frame works with distance metric functions. (We’ll talk more about distance and dissimilarity metrics in upcoming labs.)\n\nclust &lt;- rioja::chclust(dist(sqrt(props)),\n                        method = \"coniss\")\n\nplot &lt;- rioja::strat.plot(props[,-1] * 100, yvar = props$age,\n                  title = devils_samples$sitename[1],\n                  ylabel = \"Calibrated Years BP\",\n                  #xlabel = \"Pollen (%)\",\n                  y.rev = TRUE,\n                  clust = clust,\n                  wa.order = \"topleft\", scale.percent = TRUE)\n\nrioja::addClustZone(plot, clust, 4, col = \"red\")\n\n\n\n5.1.2 Change Taxon Distributions Across Space and Time\nThe true power of Neotoma is its ability to support large-scale analyses across many sites, many s time periods within sites, many proxies, and many taxa. As a first dipping of our toes in the water, lets look at temporal trends in abundance when averaged across ites. We now have site information across Michigan, with samples, and with taxon names. Let’s say we are interested in looking at the distributions of the selected taxa across time, their presence/absence:\n\ntaxabyage &lt;- allSamp %&gt;%\n  dplyr:::filter(variablename %in% c(\"Pinus\", \"Betula\", \"Quercus\",\n                             \"Tsuga\", \"Ulmus\", \"Picea\"),\n                             age &lt; 11000) %&gt;%\n  group_by(variablename, \"age\" = round(age * 2, -3) / 2) %&gt;%\n  summarise(n = length(unique(siteid)), .groups = 'keep')\n\nsamplesbyage &lt;- allSamp %&gt;%\n  dplyr::filter(variablename %in% c(\"Pinus\", \"Betula\", \"Quercus\",\n                             \"Tsuga\", \"Ulmus\", \"Picea\")) %&gt;%\n  group_by(\"age\" = round(age * 2, -3) / 2) %&gt;%\n  summarise(samples = length(unique(siteid)), .groups = 'keep')\n\ngroupbyage &lt;- taxabyage %&gt;%\n  inner_join(samplesbyage, by = \"age\") %&gt;%\n  mutate(proportion = n / samples)\n\nggplot(groupbyage, aes(x = age, y = proportion)) +\n  geom_point() +\n  geom_smooth(method = 'gam',\n              method.args = list(family = 'binomial')) +\n  facet_wrap(~variablename) +\n  #coord_cartesian(xlim = c(22500, 0), ylim = c(0, 1)) +\n  scale_x_reverse() +\n  xlab(\"Proportion of Sites with Taxon\") +\n  theme_bw()\n\nWe can see clear patterns of change for at least some taxa, and the smoothed surfaces are modeled using Generalized Additive Models (GAMs) in R, so we can have more or less control over the actual modeling using the gam or mgcv packages. Depending on how we divide the data we can also look at shifts in altitude, latitude or longitude to better understand how species distributions and abundances changed over time in this region.\nNote that for some taxa, they always have a few pollen grains in all pollen samples, so this ‘proportion of sites with taxon’ isn’t very informative. Calculating a metric like average abundance might be more useful.\nExercise question 7: Repeat the above example, for a different state or other geographic region of your choice.\n\nData checks\nTidying data\nPlotting data",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data wrangling and visualisations</span>"
    ]
  },
  {
    "objectID": "04-neotoma2.html#part-3-exercises",
    "href": "04-neotoma2.html#part-3-exercises",
    "title": "4  The neotoma2 package",
    "section": "4.5 Part 3 Exercises",
    "text": "4.5 Part 3 Exercises\n\nHow many sites have the name ‘clear’ in them? Show both your code and provide the total count.\nWhich state has more sites in Neotoma, Minnesota or Wisconsin? How many in each state? Provide both code and answer.\nHow many different kinds of datasets are available at Devil’s Lake, WI? Show both code and answer. Ensure that your code just retrieves datasets for just this single site.\nFollow the Pinus example above, but now for Picea. How many taxon names were aggregated into your Picea name?\nMake a stratigraphic pollen diagram in rioja, for a site of your choice (not Devils Lake) and taxa of your choice. Show code and resulting diagram.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>The `neotoma2` package</span>"
    ]
  }
]