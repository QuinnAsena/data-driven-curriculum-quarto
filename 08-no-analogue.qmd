# Novelty and No Analogue Futures

## Part 1: Background

The modern analog technique (MAT) is a standard approach for making quantitative inferences about past vegetation and climate from fossil pollen data – and from many other forms of paleoecological data [@chevalier2020; @williams2008; @gavin2003; @overpeck1985]. The MAT relies on reasoning by analogy [@jackson2004], in which it is assumed that two compositionally similar pollen assemblages were produced by similar plant communities, growing under similar environments. The similarity of pollen assemblages is determined by the calculation of dissimilarity metrics.

The MAT can also be thought of as a non-parametric statistical technique, that computer scientists call a k-nearest neighbors algorithm. It is a simple form of machine learning. Each fossil pollen assemblage is matched to 1 or more ($k$) modern pollen assemblages, then is assigned the ecological and environmental characteristics associated with those modern analogues.

The MAT is a popular approach for reconstructing past environments and climates, due to its relative simplicity and intuitive appeal. However, like any algorithm, if used unwisely, it can produce spurious results. More generally, when reasoning by analogy, be careful! Analogies are always incomplete and imperfect, and you must use your critical judgment to determine whether these imperfections are minor or serious.

To reconstruct past environments using the MAT, we need three data sets:

1. modern species assemblages

2. modern environmental variables

3. fossil species assemblages

The MAT follows four basic steps:

1. Calculate dissimilarities between a fossil species assemblage i and all modern species assemblages in set $S$.

2. Discard modern samples with dissimilarities greater than a threshold Dthreshold.

3. Identify and retain the k closest modern analogs.

4. Average the environmental conditions associated with the modern analogs and assign them to fossil sample $i$.

Note that we are taking a detour into paleoclimatology for two reasons. First, because paleoclimatic reconstructions are still a primary use for fossil pollen and other micropaleontological data. Second, because the MAT and the analogue package, as a dissimilarity-based approach, also lets us explore the novelty of past communities - perhaps of more interest to paleoecologists than inferred paleoclimates!

## Part 2: Cross-Validation

Before reconstructing environmental variables from fossil pollen assemblages, we usually assess how MAT performs in the modern species assemblage. This step is usually referred to as calibration or cross-validation.

In cross-validation, a calibration function is trained on one portion of the modern species assemblages (calibration set) and applied to another portion of the modern species assemblages (validation set). These two datasets are thus mutually exclusive and - possibly - independent. To cross-validate MAT, we usually use a technique called k-fold cross-validation. In $k$-fold cross-validation the modern data set is split into k mutually exclusive subsets. For each kth subset, the calibration dataset comprises all the samples not in $k$, while the samples in $k$ comprise the validation dataset. The simplest form of $k$-fold cross-validation is the leave-one-out (LOO) technique, in which just a single sample is removed and then all other samples are used to build a prediction for that sample. This procedure is then repeated for all samples. The analogue package uses leave-one-out cross-validation.

Standard metrics include RMSE and R^2. Here, we’ll use the cross-validation tools built into the analogue package.

Note that the `palaeoSig` package, developed by Richard Telford, has additional functions that can test for significance relative to randomly generated variables with the same spatial structure as the environmental variable of interest. We won’t use this package in this lab, but it’s useful for testing whether apparently strong cross-validation results are merely an artifact of spatial autocorrelation [@telford2005].

We will use the North American Modern Pollen Dataset [@whitmore2005] for this cross-validation analysis. Note that the NAMPD is pre-loaded into the analogue package, but here we’ll use use the original (complete) data so that we can filter locations easily.

Let’s get the data ready:

::: {.callout-note}
# Packages required for this section
List of packages required for section and what they are used for

```{r packages}
#| output: false
# Load up the package

if (!require("pacman")) install.packages("pacman", repos="http://cran.r-project.org")
pacman::p_load(neotoma2, tidyr, dplyr, ggplot2, neotoma2, analogue, vegan, readxl, fuzzyjoin)
```

:::

The following code (unfold by clicking the dropdown) downloads the data from the neotoma API and formats it to long data. More easily, you can read in the .rds file in the data directory.

```{r DevilsRespawnNa}
#| eval: false
#| code-fold: true
# The following code can be done in one pipeline but we have split it into chunks to help with readablity
devils_samples <- get_sites(siteid = 666) %>% # Download the site samples data
  get_downloads() %>%
  samples()

devils_samples <- devils_samples %>%
  dplyr::filter(ecologicalgroup %in% c("UPHE", "TRSH"), # Filter ecological groups by upland heath and trees and shrubs
                elementtype == "pollen", # filter bu pollen samples
                units == "NISP") %>% # Filter by sampling unit
         mutate(variablename = replace(variablename,
                                stringr::str_detect(variablename, "Pinus.*"), # Harmonize Pinus into one group
                                "Pinus"),
                variablename = replace(variablename,
                                stringr::str_detect(variablename, "Ambrosia.*"),
                                "Ambrosia")) %>%
  group_by(siteid, sitename,
           sampleid, variablename, units, age,
           agetype, depth, datasetid,
           long, lat) %>%
  summarise(value = sum(value), .groups='keep') # The group_by function will drop columns not used as grouping variables

devils_samples <- devils_samples %>% # Calculate proportions of each species by year group
  group_by(age) %>%
  mutate(pollencount = sum(value, na.rm = TRUE)) %>%
  group_by(variablename) %>%
  mutate(prop = value / pollencount) %>%
  arrange(desc(age)) %>%
  ungroup()

saveRDS(devils_samples, "./data/devils_samples.rds") # Save the data for later
```

To read in the example data use:

```{r DevilsRespawnRDSNa}
#| output: false
# Load up the data
devils_samples <- readRDS("./data/devils_samples.rds")

```

We will use the North American Modern Pollen Database (NAMPD) to create modern analogues. The NAMPD is not (yet) accessible through neotoma but is stored on servers at the University of Madison WI. **JACK ACTION POINT: to use this method in other regions what databases are available.** The following code downloads the NAMPD from the server to your data directory, unzips the downloaded data, and reads in the relevant datasheets from the excel file. If you do not already have a directory named data from previous chapters **CROSSREF INTRO set-up instructions** one will be created.


```{r nampd}
#| warning: false

dir.create("./data") # If the data directory exists it will not be overwritten
download.file("https://williamspaleolab.github.io/data/whitmoreetal2005.zip", "./data/whitmoreetal2005.zip")
unzip("./data/whitmoreetal2005.zip", exdir = "./data/whitmoreetal2005")
mod_pol <- read_excel("./data/whitmoreetal2005/whitmoreetal2005_v1-72.xls", sheet = "POLLEN DATA")
mod_clim <- read_excel("./data/whitmoreetal2005/whitmoreetal2005_v1-72.xls", sheet = "CLIMATE+BIOCLIMATE")

```

::: {.callout-warning}
# Oh no! :scream:

If you do not already have a directory named data from previous chapters **CROSSREF INTRO set-up instructions** one will be created. If the directory already exists you will see a warning message:

```bash
Warning message:
In dir.create("./data") : '.\data' already exists
```
Don't worry, the function `dir.create()` will not overwrite any existing directory contents. **./ and .\ between mac/pc/linux**

:::

Still with me? Great! Now we have our three datasets: (i) our site of interest (Devil's lake); (ii) our modern species assemblages; and (iii) our modern climate. The modern datasets are for the whole of North America and need to be filtered for the Eastern US so that the taxa are relevant to the taxa in Devil's Lake. We're first going to filter by longitude, then select by taxa so that we have a matching matrix of species between Devil's Lake and the NAMPD.


```{r, wrangle}

# several columns contain data not used for the MAT
mod_pol_east <- mod_pol %>% 
  filter(LONDD >= -105) %>% # Filter for sites east of -105 degrees
  select(ID2, 14:147) # Select colums with ID and species only

# Many functions require site-by-species matrices in wide format
# Note we pivoted by age so we have an age column not used in MAT
devils_wide <- devils_samples %>% 
  pivot_wider(id_cols = age, names_from = variablename, values_from = prop)

# Store ages separately for plotting later:
devils_ages <- devils_wide$age

```

Right, now is the tricky part of matching species names. Unfortunately, the species names between data downloaded from Neotoma and the NAMPD do not match exactly. Sometimes matching is easiest done manually but because of the number of species in both datasets we will do as much as possible programatically. The `fuzzyjoin` package allows us to match strings (names) by a distance measure and outputs a number of possible matches.


::: {.panel-tabset}

## String matching code

looks like it's done a good job for the most part!

```{r fuzzyjoin}

# Save the names of each dataset
mod_pol_names <- as_tibble(tolower(colnames(mod_pol_east[-1])))
devil_names <- as_tibble(tolower(colnames(devils_wide[-1])))

# match the names using fuzzy matching
names_matches <- stringdist_left_join(devil_names, mod_pol_names, by = "value",
                                      max_dist = 4, distance_col = "distance", method = "jw") %>% 
                   group_by(value.x) %>%
                   slice_min(order_by = distance, n = 1)

head(names_matches, 15) 
```

## Let's take a closer look
```{r}

filter(names_matches, distance > 0.1)

```
## Even closer!

```{r}

names_matches %>%
  filter(distance < 0.3)  %>% # everything above 0.3 looks like a bad match
  arrange(desc(distance)) %>% 
  print(n = 15)
```

:::

Ok looks like we only have a few to weed out manually:

```{r}
names_matches_filter <- names_matches %>% 
  filter(distance < 0.3,
         value.x != "rhus" & value.y != "rubus",
         value.x != "morus" & value.y != "cornus",
         value.x != "ericaceae" & value.y != "arecaceae",
         value.x != "ilex" & value.y != "piceax")  %>% 
         arrange(desc(distance))

```


Now we have 


## Part 3: Exercises


