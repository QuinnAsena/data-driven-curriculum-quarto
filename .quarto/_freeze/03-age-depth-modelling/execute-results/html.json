{
  "hash": "3efb19e29fbd7079ec82c3dfbe02144a",
  "result": {
    "markdown": "# Age-depth modelling {#sec-age-depth}\n\n## Part 1: Background\n\nA foundational difference between geology and ecology is that, for geologists, time is an unknown variable that must be estimated with uncertainty. In contrast, most ecologists can assume that the temporal coordinates of their observations are known precisely, with zero uncertainty. Fortunately, geochronologists have a wide variety of natural clocks, thanks to the constant decay rates of radioactive isotopes. Each isotope has a unique decay rate, and so each is appropriate for particular timescales.\n\nFor the last 50,000 years, radiocarbon (^14^C), with its half-life of 5,730 years, is by far the most common form of radiometric dating. (Beyond 10 half-lives, so much of a radioactive substance has decayed away that it becomes immeasurable.) Radiocarbon is the mainstay of Quaternary dating and archaeology.\n\nIn Quaternary paleoecology, radiocarbon dating is expensive – a single sample typically costs $300 to $500 – so usually a given lake-sediment record will have only a scattering (ca. 5 to 30) of radiocarbon dates and other age controls. Other kinds of age controls include volcanic ash layers (tephras), ^210^Pb (half-life: 22.6 yrs), optically stimulated luminescence (OSL) dates, historic events such as the rise in *Ambrosia* pollen abundances associated with EuroAmerican land clearance, etc. An age-depth model must be constructed to estimate the age of sediments not directly associated with an age control. In multi-site data syntheses, the number of age controls, their precision, and their likely accuracy are all fundamental indicators of data quality [e.g. @blois2011; @mottl2021].\n\nTo estimate ages for depths lacking radiocarbon date, an age-depth model is required. Age-depth models are fitted to the available age controls (age estimates with uncertainty for individual depths) and provide estimates of age as a function of depth, for all depths and ages within the temporal bounds of the model.\n\nHere we will gain practice in working with age-depth models of various kind, and assessing their age estimates and uncertainty. We’ll begin with a bit of practice in calibrating radiocarbon years to calendar years and comparing the resulting age estimates from different calibration curves. \n\n\n::: {.callout-note}\n# Packages required for this section\n\nWe will be using `Bchron` for calibration and Bayesian age-depth modelling. Notably `rbacon` is another commonly used package, see @trachsel2017 for a discussion on age-depth models. We will also be fitting some interpolation and linear models using BASE-R.\n\n\n::: {.cell hash='03-age-depth-modelling_cache/html/packages_fdccbbee35431c797533bb2c728c91c8'}\n\n```{.r .cell-code}\n# Load up the package\nif (!require(\"pacman\")) install.packages(\"pacman\", repos=\"http://cran.r-project.org\")\npacman::p_load(Bchron, splines) # splines comes with BASE-R\n```\n:::\n\n:::\n\n\n### Calibration of Radiocarbon Dates\n\nA complication in radiocarbon dating is that the initial calculation of a radiocarbon age assumes, by convention, that the amount of radiocarbon in the atmosphere is constant over time. See Bronk Ramsey (2008) for a good overview of ^14^C dating. This assumption is untrue, so all radiocarbon age estimates must be post-hoc calibrated using a calibration curve that is based on compiling radiocarbon dates of materials that have precise independent age estimates (e.g. tree rings, corals). The IntCal series (IntCal04, IntCal09, IntCal13, IntCal20) is the community standard for calibrating radiocarbon dates to age estimates in calendar years [e.g., @reimer2020]. The conversion from radiocarbon to calendar years usually further increases the uncertainty of age estimates.\n\nYet another complication in radiocarbon dating is that different calibration curves need to be used for the Northern vs. Southern Hemisphere and for the atmosphere vs. oceans, due to different residence times of ^14^C in these different reservoirs. For example, atmospheric radiocarbon that diffuses into the surface oceans usually will reside for centuries before phytoplankton biologically fix it through photosynthesis, which will lead  marine ^14^C to be depleted (and ‘too old’) relative to atmospheric ^14^C. Use the wrong calibration curve and your age estimate will be inaccurate!\n\n\n#### Calibrating radiocarbon dates in R \n\nHere we’ll experiment with calibrating radiocarbon dates, using various calibration curves. Radiocarbon dated samples come back from the lab with a radiocarbon age and standard deviation, among other information. These two bits of information are used to calibrate the radiocarbon dates to estimated ages. R packages may have useful vignettes (package tutorials) and built-in datasets that provide handy test templates. The following code is modified from the `Bchron` [vignette](https://cran.r-project.org/web/packages/Bchron/vignettes/Bchron.html).\n\n::: {.panel-tabset}\n\n## Code\n\n\n::: {.cell hash='03-age-depth-modelling_cache/html/BchronCalibrate_a21a845fb608127cab03df28a1670974'}\n\n```{.r .cell-code}\nages = BchronCalibrate(ages=c(3445,11553,7456), \n                        ageSds=c(50,230,110), \n                        calCurves=c('intcal20','intcal20','intcal20'))\n```\n:::\n\n\n## Summary\n\n\n::: {.cell hash='03-age-depth-modelling_cache/html/BchronCalibrateSummary_fd345464d605ee5366160c1f57d54b90'}\n\n```{.r .cell-code}\nsummary(ages)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n95% Highest density regions for Date1\n$`94.4%`\n[1] 3572 3834\n\n\n95% Highest density regions for Date2\n$`0.4%`\n[1] 13004 13025\n\n$`77.9%`\n[1] 13059 13874\n\n$`16.4%`\n[1] 13923 14012\n\n\n95% Highest density regions for Date3\n$`94.6%`\n[1] 8022 8423\n```\n:::\n:::\n\n\n## Plots\n\n\n::: {.cell hash='03-age-depth-modelling_cache/html/BchronCalibratePlot_4f4e7516824cca68ff4f25f622646ab5'}\n\n```{.r .cell-code}\nplot(ages)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[[1]]\n```\n:::\n\n::: {.cell-output-display}\n![](03-age-depth-modelling_files/figure-html/BchronCalibratePlot-1.png){width=672}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n\n[[2]]\n```\n:::\n\n::: {.cell-output-display}\n![](03-age-depth-modelling_files/figure-html/BchronCalibratePlot-2.png){width=672}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n\n[[3]]\n```\n:::\n\n::: {.cell-output-display}\n![](03-age-depth-modelling_files/figure-html/BchronCalibratePlot-3.png){width=672}\n:::\n:::\n\n\n## Calibration curve plots \n\n\n::: {.cell hash='03-age-depth-modelling_cache/html/BchronCalibrateCalPlot_1be9b4aefead343c4175d77072d22f9e'}\n\n```{.r .cell-code}\nplot(ages, includeCal = TRUE, fillCol = 'red')\n```\n\n::: {.cell-output-display}\n![](03-age-depth-modelling_files/figure-html/BchronCalibrateCalPlot-1.png){width=672}\n:::\n:::\n\n\n:::\n\nThe output summary indicates the range of the highest density regions (i.e., the most likely 'real' age range of the sample). The `plot` function in `Bchron` outputs a `ggplot` object of the high density regions of the most likely ages.\n\n### Types of Age-Depth Models\n\nDifferent kinds of age-depth models exist, each with their own underlying assumptions and behavior. In the list below, #1-4 are classical or traditional forms of age-depth models, but Bayesian models are now the norm. The packages `rbacon` (usually referred to as 'bacon') and `Bchron` are the current standards for Bayesian age-depth modelling. Before going to bayesian models, we’ll begin with the classics.\n\n1. **Linear interpolation**, a.k.a. ‘connect the dots,’ in which straight lines are drawn between each depth-adjacent pair of age controls.\n\n2. **Linear regression** ($y=b0~ + b1x$; $y=$time and $x=$depth; $b0$ and $b1$ are constants), in which a single straight line is fitted through the entire set of age controls. In ordinary linear regression (OLS), the chosen line will minimize the y-dimension distances of individual points to the line. Standard OLS assumes that all observations are normally distributed, which is a poor assumption for calibrated radiocarbon dates.\n\n3. **Polynomials**, also fitted to the entire set of age controls ($y= b0 + b1x + b2x^2 + b3x^3 + …bnx^n$), are an extension of linear regression, with additional terms for $x^2$, $x^3$, etc. Some arbitrary maximum n is chosen, usually in the range of 3 to 5. These are called 'third-order polynomials,' 'fifth-order polynomials,' etc.\n\n4. **Splines**, which are a special kind of polynomial function that are locally fitted to subsets of the age controls, and then smoothly interpolated between points. (Several different formulas can be used to generate splines; common forms include cubic, smooth, monotonic, and LOWESS).\n\n5. **Bayesian age models** (e.g. `bacon`, `bchron`, `oxcal`, etc.). Bayesian models differ in detail, but all combine a statistical representation of prior knowledge with the new data (i.e. the age controls at a site) to build an age-depth model with posterior estimates of age for any given depth. Bayesian models are now widely used because\n    i) they allow the incorporation of prior knowledge (e.g., from coring many lakes, we now have decent estimates of typical sediment accumulation rates, @goring2012);\n    ii) they can handle highly irregular probability distribution functions such as those for radiocarbon dates after calibration; and as a result\n    iii)  they generally do a better job of describing uncertainty than traditional age-depth models.\n\n\n#### Classical age-depth models\n\nClassical models are now out-dated methods [@blaauw2018], but it is useful to understand how they work as literature before the relatively recent development of Bayesian methods has relied on them. Let's explore some classical methods of age-depth modelling using one of the datasets included with the `Bchron` package. The data are from a core in Northern Ireland; Sluggan Bog @smith1991, and can be called via:\n\n\n::: {.cell hash='03-age-depth-modelling_cache/html/Classical_a1792ae713f6ec7d77d18cf13128e133'}\n\n```{.r .cell-code}\ndata(Sluggan) # Call the data from Bchron\nprint(Sluggan) # Check out the data\n```\n:::\n\n\n::: {.panel-tabset}\n\n## Linear interpolation\n\nLinear interpolation predicts ages by simply, drawing a line between successive dated samples. This method assumes that there is a constant age-depth relationship between samples. An assumption that is unlikely to be true, especially of cores with fewer dated samples than Sluggan Moss.\n\n\n::: {.cell hash='03-age-depth-modelling_cache/html/LinearInterp_31a30a9b3a6326a931ced14f1efdc65d'}\n\n```{.r .cell-code}\ninterp_ages <- approx(x = Sluggan$ages, y = Sluggan$position) # use the function approx() to interpolate between ages\nplot(x = interp_ages$x, y = interp_ages$y, type = 'l') # Plot the interpolated data\npoints(x = Sluggan$ages, y = Sluggan$position, cex = 1.5, col = 'red') # overlay the original age points\n```\n\n::: {.cell-output-display}\n![](03-age-depth-modelling_files/figure-html/LinearInterp-1.png){width=672}\n:::\n:::\n\n\n## Linear regression\n\nLinear regression provides a line of best fit through the dated samples. This method assumes a constant age-depth relationship across all samples, also unlikely to be true depending on processes affecting the core during its formation.\n\n\n::: {.cell hash='03-age-depth-modelling_cache/html/LinearReg_eca9c62b0657bfc8d610c896fe7de67a'}\n\n```{.r .cell-code}\nmod_ages <- lm(Sluggan$position ~ Sluggan$ages) # Create a linear regression model\nplot(x = Sluggan$ages, y = Sluggan$position, cex = 1.5, col = 'red') # Plot the original ages\nabline(mod_ages) # add the regression line from the regression model\n```\n\n::: {.cell-output-display}\n![](03-age-depth-modelling_files/figure-html/LinearReg-1.png){width=672}\n:::\n:::\n\n\n## Polynomial regression\n\nPolynomial regression allows a curve to be fit through the data. The amount the curve 'wiggles' depends on the order of the polynomial fit to the data. Polynomial regression has the risk of being over-fit.\n\n\n::: {.cell hash='03-age-depth-modelling_cache/html/PolyReg_be4fdab94892fb47a3a3cdafc9aefa48'}\n\n```{.r .cell-code}\nx <- Sluggan$ages # Renaming the variables because the predict function below is fussy about the input name\ny <- Sluggan$position\npoly_ages <- lm(y ~ poly(x, 3))\n\nplot(x = Sluggan$ages, y = Sluggan$position, cex = 1.5, col = 'red')\nage_range <- seq(from = range(Sluggan$ages)[1], to = range(Sluggan$ages)[2], length.out = 250)\nlines(age_range, predict(poly_ages, data.frame(x = age_range)))\n```\n\n::: {.cell-output-display}\n![](03-age-depth-modelling_files/figure-html/PolyReg-1.png){width=672}\n:::\n:::\n\n\n## Cubic splines\n\nSplines are a class of functions including, for example, smoothing splines or cubic splines.  Cubic splines are pieve-wise polynomials locally between 'knots'. That is the data are split into bins that are fit independently using. By default, the `bs()` function uses a third degree polynomial. Without providing knots the fit will look the same as a third degree polynomial regression.\n\n\n::: {.cell hash='03-age-depth-modelling_cache/html/CubicSpline_d5495c5a5dfd61791850d754c2e56183'}\n\n```{.r .cell-code}\ncubic_ages <- lm(y ~ bs(x, knots = c(1000, 6000, 12000)))\nplot(x = Sluggan$ages, y = Sluggan$position, cex = 1.5, col = 'red')\nlines(age_range, predict(cubic_ages, data.frame(x = age_range)))\n```\n\n::: {.cell-output-display}\n![](03-age-depth-modelling_files/figure-html/CubicSpline-1.png){width=672}\n:::\n:::\n\n\n:::\n\nBecause classical age-depth modelling is rarely used now we are not going to delve further into the statistical details of the best way of fitting each model (e.g., the number of knots to use for fitting a cubic spline). \n\nOne of the issues with classical age-depth modelling is that uncertainty _decreases_ with fewer datapoints.\n\n\n#### Bayesian age-depth models\n\nNow let's see what the latest methods show for the same dataset.\n\n:::  {.panel-tabset}\n\n## Code\n\nNote that all the values provided to the arguments are contained in the Sluggan dataframe. When creating chronologies from your own (or accessed data), you may need to rename them to match your data.\n\n\n::: {.cell hash='03-age-depth-modelling_cache/html/Bayesian_d089ac12966b77e57fed5212fa0665a8'}\n\n```{.r .cell-code}\nSlugganOut = with(Sluggan, \n               Bchronology(ages=ages,\n                           ageSds=ageSds, \n                           calCurves=calCurves,\n                           positions=position, \n                           positionThicknesses=thickness,\n                           ids=id, \n                           predictPositions=seq(0,518, by=10)))\n```\n:::\n\n\n## Summary\n\nThe summary shows for each position (depth) the median and quartiles of the predicted ages for that position.\n\n\n::: {.cell hash='03-age-depth-modelling_cache/html/BayesianSummary_ad8bc5e0dfb560b6a0bef365314a681e'}\n\n```{.r .cell-code}\nsummary(SlugganOut)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nQuantiles of predicted ages by depth: \n Depth      2.5%      25%     50%      75%     97.5%\n     0   -59.025    71.75   211.0   383.25   705.000\n    10    32.000   223.00   373.0   528.25   772.025\n    20   127.975   369.00   509.0   645.00   837.025\n    30   274.000   525.00   643.0   748.00   881.050\n    40   484.000   709.00   786.0   852.25   929.000\n    50   963.000  1071.00  1131.0  1198.00  1331.075\n    60  1145.000  1273.00  1333.0  1401.00  1540.075\n    70  1400.950  1486.00  1552.0  1616.00  1758.125\n    80  1515.000  1661.00  1741.0  1842.00  2021.125\n    90  1638.000  1822.00  1908.0  1996.25  2156.150\n   100  1839.925  2014.00  2080.5  2157.25  2273.000\n   110  2175.950  2367.00  2473.0  2622.50  2907.125\n   120  2427.925  2710.00  2825.5  2941.25  3138.050\n   130  2967.000  3149.00  3253.5  3382.00  3724.125\n   140  3130.900  3368.75  3521.0  3703.00  4033.000\n   150  3284.975  3615.75  3807.0  3973.25  4244.200\n   160  3582.950  3966.00  4104.5  4210.00  4384.075\n   170  4187.975  4338.75  4418.0  4495.25  4634.025\n   180  4319.975  4489.00  4562.0  4628.25  4749.000\n   190  4517.900  4667.00  4742.0  4813.00  4931.050\n   200  4593.000  4758.00  4831.0  4900.00  5037.000\n   210  4681.950  4835.00  4915.5  4984.00  5126.000\n   220  4765.000  4928.00  4996.0  5068.00  5207.025\n   230  4906.950  5042.00  5095.0  5162.00  5304.025\n   240  5535.975  5634.00  5680.0  5728.00  5862.025\n   250  5641.000  5737.00  5792.0  5852.25  5973.200\n   260  5703.000  5832.00  5887.0  5939.00  6056.025\n   270  5859.000  5946.00  5994.5  6037.00  6127.025\n   280  6287.975  6489.00  6625.0  6819.25  7255.125\n   290  6540.975  6966.75  7161.0  7319.00  7534.050\n   300  7499.000  7612.00  7687.0  7777.25  8083.025\n   310  7652.000  7824.00  7952.5  8106.00  8397.075\n   320  7798.000  8063.75  8216.0  8345.50  8579.000\n   330  8156.875  8391.00  8473.0  8577.25  8764.025\n   340  8472.975  8606.00  8690.0  8777.25  8944.050\n   350  8547.825  8702.00  8787.0  8874.00  9027.025\n   360  8657.725  8805.75  8896.5  8977.00  9120.000\n   370  9077.000  9221.00  9288.0  9368.00  9489.050\n   380  9168.975  9323.00  9393.0  9475.25  9628.225\n   390  9248.950  9402.75  9476.0  9564.00  9759.050\n   400  9344.000  9474.00  9566.0  9663.25  9871.075\n   410  9506.975  9647.00  9760.5  9891.00 10116.075\n   420  9639.925  9865.50  9986.0 10106.00 10259.050\n   430  9870.800 10082.75 10260.5 10317.25 10456.100\n   440  9981.925 10192.75 10301.0 10377.00 10527.150\n   450 10410.975 10568.75 10671.0 10776.00 11040.050\n   460 10721.975 10922.00 11033.0 11118.25 11246.025\n   470 11200.975 11514.50 11716.0 11957.00 12389.125\n   480 11619.900 12200.75 12421.0 12542.00 12712.025\n   490 12747.975 12865.00 12920.5 12987.00 13106.075\n   500 12931.000 13051.00 13119.0 13187.00 13389.250\n   510 13338.925 13504.00 13598.0 13701.25 13937.050\n```\n:::\n:::\n\n\n## Plot\n\n\n::: {.cell hash='03-age-depth-modelling_cache/html/BayesianPlots_0066df0e586e5a8197eabaac02969daf'}\n\n```{.r .cell-code}\nplot(SlugganOut)\n```\n\n::: {.cell-output-display}\n![](03-age-depth-modelling_files/figure-html/BayesianPlots-1.png){width=672}\n:::\n:::\n\n\n:::\n\nA more complete version of the modelling process using Bchron can be found in the [vignette](https://cran.r-project.org/web/packages/Bchron/vignettes/Bchron.html). \n\n::: {.callout-tip}\n# Resources\n\nAge-depth modelling is complicated, there are many pitfalls, assumptions, and uncertainties that are often ignored. Recent developments have begun to focus on quantifying uncertainties to understand the reliability of inferences made from the data. Ket papers for understanding age depth modelling include:\n\n- @blaauw2018\n\n- @parnell2008\n\n- @trachsel2017\n\n:::\n\n## Part 2: Exercises\n\nYou may want to refer to the bchron vignette to find some helpful information for the exercises below.\n\n### Calibration\n\n1. Using the existing example code, change the `calCurves` input to:\n    i) the Southern Hemisphere calibration curve\n    ii) the marine sediments curve.\n\n2. Calibrate a different set of three ages by modifying the inputs to `ages` and `ageSds`.\n\nThe object you assigned the result of `BchronCalibrate` to contains all the calibration information and can be accessed with the `$` e.g., `ages$Date1$ageGrid`\n\n3. Using one of your calibrated sets of three dates, calculate the median age from the `ageGrid` and the difference of each calibrated age to the original uncalibrated age.\n\n::: {.callout-note}\n# Median and mean ages\n\nCommonly, only the median age is reported; however, it is good practice to report and consider the range of possible ages, especially when exploring synchroneity of events across space. Notice how `Bchron` focuses on delivering ranges and densities.\n\n:::\n\n### Classical age-depth modelling:\n\n1. `Bchron` has a second built-in dataset named Glendalough. Using the Glendalough dataset:\n    i) Fit a linear interpolation, linear regression, three different order polynomials and a cubic spline\n    ii) Plot the results\n    \n2. Did you run into any errors or warnings? If so, why?\n\n### Bayesian age-depth modelling\n\n1. Run the `Bchron` age-depth model on the Glendalough dataset and consider:\n    i) How does uncertainty change between the plot outputs between the Glendalough (a dataset with fewer radiocarbon dates) and Sluggan Moss (a data-rich site)\n    ii) Run the summary funcion on the Glendalough model, does the output reflect the plot? Is there smaller uncertainty around samples? Why is interpreting results in the context of the uncertainty important?\n\n## Learning outcomes (Rephrase/exclude)\n\n- [ ] Learn how to access and use-built in datasets\n\n- [ ] Learn how to find package vignettes\n\n# References {.unnumbered}\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}